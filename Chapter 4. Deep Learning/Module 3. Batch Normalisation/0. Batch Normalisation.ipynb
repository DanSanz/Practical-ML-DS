{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "- Formula behind batch normalisation\n",
    "- Application in PyTorch\n",
    "- Advantage of batch normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "- Data normalisation\n",
    "- Feed-forward neural networks\n",
    "- Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "Before training a machine learning algorithm, it is common practice to normalise the input data, especially when the features have different scales, e.g. house prices and building year. This can lead to features with higher values having an unwanted greater impact on changes of a predictor. Normalising data can avoid this and lead to better performance. Since this technique is proven to work for the input data, it is natural to apply the same technique to the hidden layers in a neural network. <br>\n",
    "Batch normalisation tackles the problem of internal covariate shift in deep neural networks, which describes the phenomenon that the input distribution of each layer changes a lot as the input to each layer is affected by the parameters of all preceding layers s.t. even small changes in parameters have a big impact on the parameters further down the line. Solving this problem typically requires careful initialisation of the parameters as well as small learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Notation\n",
    "Batch normalisation basically sets the mean of each feature to zero and the variance to 1:\n",
    "\\begin{equation*}\n",
    "\\hat{x} = \\frac{x - \\mu(x)}{\\sqrt{\\sigma^2(x)}}\n",
    "\\end{equation*}\n",
    "where the mean and variance are computed over the batch during training and over the population after training.<br>\n",
    "The normalised features are then scaled and shifted by introducing the parameters $\\gamma$ and $\\beta$:\n",
    "\\begin{equation*}\n",
    "y = \\gamma \\hat{x} + \\beta\n",
    "\\end{equation*}\n",
    "These steps are applied to the activations of each layer before feeding them as input to the next layer. Therefore, we must also include them in the backpropagation, which we do by calculating the gradient w.r.t. the new parameters $\\gamma$ and $\\beta$. \n",
    "These parameters are important because the computation of $\\hat{x}$ where the mean is set to 0 with unit variance might not be desirable in every layer. Especially if we compute this for the input of a softmax function, it is desirable that the input has higher variance such that the output is a conclusive probability distribution over its input. The $\\gamma$ and $\\beta$ parameters learn to regulate $\\hat{x}$. If $\\gamma$ is set to $\\sqrt{\\sigma^2(x)}$ and $\\beta$ is set to $\\mu(x)$, we can restore the original values of $x$.\n",
    "<br><br>\n",
    "\n",
    "In a convolutional neural network, the normalisation is applied jointly over all locations in a feature map, s.t. we can learn the parameters $\\gamma$ and $\\beta$ per feature map and not per activation because we want to normalise the features in the same way regardless of whether they are in different convolutional windows. This means that we normalise the same activations in the same feature maps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "We will use the CNN for MNIST image prediction that you already worked with before. Run the original code so you can compare performance to the same CNN using batch normalisation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The tensorboard extension is already loaded. To reload it, use:\n  %reload_ext tensorboard\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 251.565 248.518125\" width=\"251.565pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <defs>\n  <style type=\"text/css\">\n*{stroke-linecap:butt;stroke-linejoin:round;}\n  </style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 248.518125 \nL 251.565 248.518125 \nL 251.565 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \nL 244.365 7.2 \nL 26.925 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g clip-path=\"url(#pd522a71124)\">\n    <image height=\"218\" id=\"imagef35fcbd98e\" transform=\"scale(1 -1)translate(0 -218)\" width=\"218\" x=\"26.925\" xlink:href=\"data:image/png;base64,\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\" y=\"-6.64\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m17bae3d9cb\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.807857\" xlink:href=\"#m17bae3d9cb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <defs>\n       <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n      </defs>\n      <g transform=\"translate(27.626607 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"69.636429\" xlink:href=\"#m17bae3d9cb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <defs>\n       <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n      </defs>\n      <g transform=\"translate(66.455179 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.465\" xlink:href=\"#m17bae3d9cb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <defs>\n       <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n      </defs>\n      <g transform=\"translate(102.1025 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"147.293571\" xlink:href=\"#m17bae3d9cb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g transform=\"translate(140.931071 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"186.122143\" xlink:href=\"#m17bae3d9cb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <defs>\n       <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n      </defs>\n      <g transform=\"translate(179.759643 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"224.950714\" xlink:href=\"#m17bae3d9cb\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g transform=\"translate(218.588214 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"md86aa7f416\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#md86aa7f416\" y=\"11.082857\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g transform=\"translate(13.5625 14.882076)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#md86aa7f416\" y=\"49.911429\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g transform=\"translate(13.5625 53.710647)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#md86aa7f416\" y=\"88.74\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g transform=\"translate(7.2 92.539219)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#md86aa7f416\" y=\"127.568571\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g transform=\"translate(7.2 131.36779)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#md86aa7f416\" y=\"166.397143\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g transform=\"translate(7.2 170.196362)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#md86aa7f416\" y=\"205.225714\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g transform=\"translate(7.2 209.024933)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 224.64 \nL 26.925 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 244.365 224.64 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 224.64 \nL 244.365 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 244.365 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd522a71124\">\n   <rect height=\"217.44\" width=\"217.44\" x=\"26.925\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=transforms.ToTensor(),          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           train=False,\n",
    "                          )\n",
    "\n",
    "x = train_data[np.random.randint(0, 300)][0]    # get a random example\n",
    "#print(x)\n",
    "plt.imshow(x[0].numpy(),cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(torch.nn.Module):\n",
    "    def __init__(self, batch_norm):\n",
    "        super().__init__()\n",
    "            # conv2d(in_channels, out_channels, kernel_size)\n",
    "            # in_channels is the number of layers which it takes in (i.e.num color channels in 1st layer)\n",
    "            # out_channels is the number of different filters that we use\n",
    "            # kernel_size is the depthxwidthxheight of the kernel#\n",
    "            # stride is how many pixels we shift the kernel by each time\n",
    "        self.batch_norm = batch_norm\n",
    "        if self.batch_norm:\n",
    "            print('yes')\n",
    "            self.conv_layers = torch.nn.Sequential( # put your convolutional architecture here using torch.nn.Sequential \n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(16),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.BatchNorm2d(32)\n",
    "            )\n",
    "            self.fc_layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(32*20*20, 10) # put your linear architecture here using torch.nn.Sequential \n",
    "        )\n",
    "        else:\n",
    "            self.conv_layers = torch.nn.Sequential( # put your convolutional architecture here using torch.nn.Sequential \n",
    "            torch.nn.Conv2d(1, 16, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(16, 32, kernel_size=5, stride=1),\n",
    "            torch.nn.ReLU()\n",
    "            )\n",
    "            self.fc_layers = torch.nn.Sequential(\n",
    "                torch.nn.Linear(32*20*20, 10) # put your linear architecture here using torch.nn.Sequential \n",
    "            )\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)# pass through conv layers\n",
    "        x = x.view(x.shape[0], -1)# flatten output ready for fully connected layer\n",
    "        x = self.fc_layers(x)# pass through fully connected layer\n",
    "        x = F.softmax(x, dim=1)# softmax activation function on outputs\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available() # checks if gpu is available\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "learning_rate = 0.0005 # set learning rate\n",
    "epochs = 5 # set number of epochs\n",
    "\n",
    "cnn = ConvNet(batch_norm=False).to(device) #.to(device)#instantiate model\n",
    "criterion = torch.nn.CrossEntropyLoss() #use cross entropy loss function\n",
    "optimiser = torch.optim.Adam(cnn.parameters(), lr=learning_rate) # use Adam optimizer, passing it the parameters of your model and the learning rate\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "writer1 = SummaryWriter(log_dir=\"runs/cnn\") # we will use this to show our models performance on a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "05005\nEpoch: 3 \tBatch: 353 \tLoss: 1.4823527336120605\nEpoch: 3 \tBatch: 354 \tLoss: 1.4692952632904053\nEpoch: 3 \tBatch: 355 \tLoss: 1.4763778448104858\nEpoch: 3 \tBatch: 356 \tLoss: 1.5029478073120117\nEpoch: 3 \tBatch: 357 \tLoss: 1.482025146484375\nEpoch: 3 \tBatch: 358 \tLoss: 1.4939398765563965\nEpoch: 3 \tBatch: 359 \tLoss: 1.5119682550430298\nEpoch: 3 \tBatch: 360 \tLoss: 1.4724559783935547\nEpoch: 3 \tBatch: 361 \tLoss: 1.4951804876327515\nEpoch: 3 \tBatch: 362 \tLoss: 1.470774531364441\nEpoch: 3 \tBatch: 363 \tLoss: 1.4798011779785156\nEpoch: 3 \tBatch: 364 \tLoss: 1.479568362236023\nEpoch: 3 \tBatch: 365 \tLoss: 1.5032215118408203\nEpoch: 3 \tBatch: 366 \tLoss: 1.500737190246582\nEpoch: 3 \tBatch: 367 \tLoss: 1.4723504781723022\nEpoch: 3 \tBatch: 368 \tLoss: 1.494850516319275\nEpoch: 3 \tBatch: 369 \tLoss: 1.4782570600509644\nEpoch: 3 \tBatch: 370 \tLoss: 1.4615105390548706\nEpoch: 3 \tBatch: 371 \tLoss: 1.5110297203063965\nEpoch: 3 \tBatch: 372 \tLoss: 1.4826791286468506\nEpoch: 3 \tBatch: 373 \tLoss: 1.4728959798812866\nEpoch: 3 \tBatch: 374 \tLoss: 1.4745405912399292\nEpoch: 3 \tBatch: 375 \tLoss: 1.4960557222366333\nEpoch: 3 \tBatch: 376 \tLoss: 1.4865154027938843\nEpoch: 3 \tBatch: 377 \tLoss: 1.4804339408874512\nEpoch: 3 \tBatch: 378 \tLoss: 1.4884133338928223\nEpoch: 3 \tBatch: 379 \tLoss: 1.46686851978302\nEpoch: 3 \tBatch: 380 \tLoss: 1.4788647890090942\nEpoch: 3 \tBatch: 381 \tLoss: 1.4802391529083252\nEpoch: 3 \tBatch: 382 \tLoss: 1.4710040092468262\nEpoch: 3 \tBatch: 383 \tLoss: 1.4819427728652954\nEpoch: 3 \tBatch: 384 \tLoss: 1.48908269405365\nEpoch: 3 \tBatch: 385 \tLoss: 1.4784730672836304\nEpoch: 3 \tBatch: 386 \tLoss: 1.4690678119659424\nEpoch: 3 \tBatch: 387 \tLoss: 1.508713960647583\nEpoch: 3 \tBatch: 388 \tLoss: 1.4882136583328247\nEpoch: 3 \tBatch: 389 \tLoss: 1.4786376953125\nEpoch: 3 \tBatch: 390 \tLoss: 1.4618116617202759\nEpoch: 4 \tBatch: 0 \tLoss: 1.477656602859497\nEpoch: 4 \tBatch: 1 \tLoss: 1.4905731678009033\nEpoch: 4 \tBatch: 2 \tLoss: 1.4786242246627808\nEpoch: 4 \tBatch: 3 \tLoss: 1.4873976707458496\nEpoch: 4 \tBatch: 4 \tLoss: 1.470674991607666\nEpoch: 4 \tBatch: 5 \tLoss: 1.5167514085769653\nEpoch: 4 \tBatch: 6 \tLoss: 1.4968576431274414\nEpoch: 4 \tBatch: 7 \tLoss: 1.4850499629974365\nEpoch: 4 \tBatch: 8 \tLoss: 1.4639090299606323\nEpoch: 4 \tBatch: 9 \tLoss: 1.4730889797210693\nEpoch: 4 \tBatch: 10 \tLoss: 1.4710010290145874\nEpoch: 4 \tBatch: 11 \tLoss: 1.4804415702819824\nEpoch: 4 \tBatch: 12 \tLoss: 1.4815157651901245\nEpoch: 4 \tBatch: 13 \tLoss: 1.4796441793441772\nEpoch: 4 \tBatch: 14 \tLoss: 1.4778316020965576\nEpoch: 4 \tBatch: 15 \tLoss: 1.4642220735549927\nEpoch: 4 \tBatch: 16 \tLoss: 1.4907965660095215\nEpoch: 4 \tBatch: 17 \tLoss: 1.520835280418396\nEpoch: 4 \tBatch: 18 \tLoss: 1.4941660165786743\nEpoch: 4 \tBatch: 19 \tLoss: 1.4818272590637207\nEpoch: 4 \tBatch: 20 \tLoss: 1.4815945625305176\nEpoch: 4 \tBatch: 21 \tLoss: 1.4879531860351562\nEpoch: 4 \tBatch: 22 \tLoss: 1.4977003335952759\nEpoch: 4 \tBatch: 23 \tLoss: 1.4903780221939087\nEpoch: 4 \tBatch: 24 \tLoss: 1.4905943870544434\nEpoch: 4 \tBatch: 25 \tLoss: 1.475903868675232\nEpoch: 4 \tBatch: 26 \tLoss: 1.475468397140503\nEpoch: 4 \tBatch: 27 \tLoss: 1.4778270721435547\nEpoch: 4 \tBatch: 28 \tLoss: 1.4790112972259521\nEpoch: 4 \tBatch: 29 \tLoss: 1.471056342124939\nEpoch: 4 \tBatch: 30 \tLoss: 1.4809343814849854\nEpoch: 4 \tBatch: 31 \tLoss: 1.4632924795150757\nEpoch: 4 \tBatch: 32 \tLoss: 1.4725017547607422\nEpoch: 4 \tBatch: 33 \tLoss: 1.4846614599227905\nEpoch: 4 \tBatch: 34 \tLoss: 1.4697158336639404\nEpoch: 4 \tBatch: 35 \tLoss: 1.4897514581680298\nEpoch: 4 \tBatch: 36 \tLoss: 1.4851784706115723\nEpoch: 4 \tBatch: 37 \tLoss: 1.4693578481674194\nEpoch: 4 \tBatch: 38 \tLoss: 1.484794020652771\nEpoch: 4 \tBatch: 39 \tLoss: 1.5076502561569214\nEpoch: 4 \tBatch: 40 \tLoss: 1.4934715032577515\nEpoch: 4 \tBatch: 41 \tLoss: 1.4680801630020142\nEpoch: 4 \tBatch: 42 \tLoss: 1.4815870523452759\nEpoch: 4 \tBatch: 43 \tLoss: 1.4692509174346924\nEpoch: 4 \tBatch: 44 \tLoss: 1.4996721744537354\nEpoch: 4 \tBatch: 45 \tLoss: 1.5071048736572266\nEpoch: 4 \tBatch: 46 \tLoss: 1.4612760543823242\nEpoch: 4 \tBatch: 47 \tLoss: 1.4874906539916992\nEpoch: 4 \tBatch: 48 \tLoss: 1.4846128225326538\nEpoch: 4 \tBatch: 49 \tLoss: 1.480773687362671\nEpoch: 4 \tBatch: 50 \tLoss: 1.4846456050872803\nEpoch: 4 \tBatch: 51 \tLoss: 1.4868173599243164\nEpoch: 4 \tBatch: 52 \tLoss: 1.4621299505233765\nEpoch: 4 \tBatch: 53 \tLoss: 1.4715197086334229\nEpoch: 4 \tBatch: 54 \tLoss: 1.4821946620941162\nEpoch: 4 \tBatch: 55 \tLoss: 1.4718947410583496\nEpoch: 4 \tBatch: 56 \tLoss: 1.4811335802078247\nEpoch: 4 \tBatch: 57 \tLoss: 1.4799857139587402\nEpoch: 4 \tBatch: 58 \tLoss: 1.467046856880188\nEpoch: 4 \tBatch: 59 \tLoss: 1.492646336555481\nEpoch: 4 \tBatch: 60 \tLoss: 1.4724348783493042\nEpoch: 4 \tBatch: 61 \tLoss: 1.4951366186141968\nEpoch: 4 \tBatch: 62 \tLoss: 1.4746466875076294\nEpoch: 4 \tBatch: 63 \tLoss: 1.4625529050827026\nEpoch: 4 \tBatch: 64 \tLoss: 1.4872255325317383\nEpoch: 4 \tBatch: 65 \tLoss: 1.4839791059494019\nEpoch: 4 \tBatch: 66 \tLoss: 1.4866397380828857\nEpoch: 4 \tBatch: 67 \tLoss: 1.4890177249908447\nEpoch: 4 \tBatch: 68 \tLoss: 1.4739102125167847\nEpoch: 4 \tBatch: 69 \tLoss: 1.4925941228866577\nEpoch: 4 \tBatch: 70 \tLoss: 1.4848910570144653\nEpoch: 4 \tBatch: 71 \tLoss: 1.4660656452178955\nEpoch: 4 \tBatch: 72 \tLoss: 1.4842151403427124\nEpoch: 4 \tBatch: 73 \tLoss: 1.4678629636764526\nEpoch: 4 \tBatch: 74 \tLoss: 1.4912493228912354\nEpoch: 4 \tBatch: 75 \tLoss: 1.4676536321640015\nEpoch: 4 \tBatch: 76 \tLoss: 1.4625070095062256\nEpoch: 4 \tBatch: 77 \tLoss: 1.493367314338684\nEpoch: 4 \tBatch: 78 \tLoss: 1.463971495628357\nEpoch: 4 \tBatch: 79 \tLoss: 1.4700926542282104\nEpoch: 4 \tBatch: 80 \tLoss: 1.4815149307250977\nEpoch: 4 \tBatch: 81 \tLoss: 1.4901891946792603\nEpoch: 4 \tBatch: 82 \tLoss: 1.466306209564209\nEpoch: 4 \tBatch: 83 \tLoss: 1.4724971055984497\nEpoch: 4 \tBatch: 84 \tLoss: 1.4883506298065186\nEpoch: 4 \tBatch: 85 \tLoss: 1.4857585430145264\nEpoch: 4 \tBatch: 86 \tLoss: 1.4696142673492432\nEpoch: 4 \tBatch: 87 \tLoss: 1.4642375707626343\nEpoch: 4 \tBatch: 88 \tLoss: 1.4786819219589233\nEpoch: 4 \tBatch: 89 \tLoss: 1.4694687128067017\nEpoch: 4 \tBatch: 90 \tLoss: 1.4777878522872925\nEpoch: 4 \tBatch: 91 \tLoss: 1.4763848781585693\nEpoch: 4 \tBatch: 92 \tLoss: 1.477268099784851\nEpoch: 4 \tBatch: 93 \tLoss: 1.497084379196167\nEpoch: 4 \tBatch: 94 \tLoss: 1.4669773578643799\nEpoch: 4 \tBatch: 95 \tLoss: 1.490977168083191\nEpoch: 4 \tBatch: 96 \tLoss: 1.475502848625183\nEpoch: 4 \tBatch: 97 \tLoss: 1.475436806678772\nEpoch: 4 \tBatch: 98 \tLoss: 1.496158242225647\nEpoch: 4 \tBatch: 99 \tLoss: 1.477996826171875\nEpoch: 4 \tBatch: 100 \tLoss: 1.5126001834869385\nEpoch: 4 \tBatch: 101 \tLoss: 1.4636461734771729\nEpoch: 4 \tBatch: 102 \tLoss: 1.4637969732284546\nEpoch: 4 \tBatch: 103 \tLoss: 1.4737039804458618\nEpoch: 4 \tBatch: 104 \tLoss: 1.4775102138519287\nEpoch: 4 \tBatch: 105 \tLoss: 1.4884567260742188\nEpoch: 4 \tBatch: 106 \tLoss: 1.4796032905578613\nEpoch: 4 \tBatch: 107 \tLoss: 1.4759718179702759\nEpoch: 4 \tBatch: 108 \tLoss: 1.4933674335479736\nEpoch: 4 \tBatch: 109 \tLoss: 1.4726662635803223\nEpoch: 4 \tBatch: 110 \tLoss: 1.466828465461731\nEpoch: 4 \tBatch: 111 \tLoss: 1.4781016111373901\nEpoch: 4 \tBatch: 112 \tLoss: 1.4820280075073242\nEpoch: 4 \tBatch: 113 \tLoss: 1.461297631263733\nEpoch: 4 \tBatch: 114 \tLoss: 1.477779746055603\nEpoch: 4 \tBatch: 115 \tLoss: 1.4766247272491455\nEpoch: 4 \tBatch: 116 \tLoss: 1.4712648391723633\nEpoch: 4 \tBatch: 117 \tLoss: 1.4870473146438599\nEpoch: 4 \tBatch: 118 \tLoss: 1.4767177104949951\nEpoch: 4 \tBatch: 119 \tLoss: 1.4832401275634766\nEpoch: 4 \tBatch: 120 \tLoss: 1.4692976474761963\nEpoch: 4 \tBatch: 121 \tLoss: 1.4817572832107544\nEpoch: 4 \tBatch: 122 \tLoss: 1.4695560932159424\nEpoch: 4 \tBatch: 123 \tLoss: 1.4789224863052368\nEpoch: 4 \tBatch: 124 \tLoss: 1.4743086099624634\nEpoch: 4 \tBatch: 125 \tLoss: 1.4752076864242554\nEpoch: 4 \tBatch: 126 \tLoss: 1.4694801568984985\nEpoch: 4 \tBatch: 127 \tLoss: 1.4757673740386963\nEpoch: 4 \tBatch: 128 \tLoss: 1.4703186750411987\nEpoch: 4 \tBatch: 129 \tLoss: 1.4867119789123535\nEpoch: 4 \tBatch: 130 \tLoss: 1.4928549528121948\nEpoch: 4 \tBatch: 131 \tLoss: 1.4766823053359985\nEpoch: 4 \tBatch: 132 \tLoss: 1.4733353853225708\nEpoch: 4 \tBatch: 133 \tLoss: 1.488571047782898\nEpoch: 4 \tBatch: 134 \tLoss: 1.4671286344528198\nEpoch: 4 \tBatch: 135 \tLoss: 1.466011881828308\nEpoch: 4 \tBatch: 136 \tLoss: 1.4914494752883911\nEpoch: 4 \tBatch: 137 \tLoss: 1.471346139907837\nEpoch: 4 \tBatch: 138 \tLoss: 1.4740149974822998\nEpoch: 4 \tBatch: 139 \tLoss: 1.4863181114196777\nEpoch: 4 \tBatch: 140 \tLoss: 1.4618321657180786\nEpoch: 4 \tBatch: 141 \tLoss: 1.4708606004714966\nEpoch: 4 \tBatch: 142 \tLoss: 1.481866717338562\nEpoch: 4 \tBatch: 143 \tLoss: 1.4794330596923828\nEpoch: 4 \tBatch: 144 \tLoss: 1.500260353088379\nEpoch: 4 \tBatch: 145 \tLoss: 1.4918025732040405\nEpoch: 4 \tBatch: 146 \tLoss: 1.47799551486969\nEpoch: 4 \tBatch: 147 \tLoss: 1.4846910238265991\nEpoch: 4 \tBatch: 148 \tLoss: 1.4953681230545044\nEpoch: 4 \tBatch: 149 \tLoss: 1.4985178709030151\nEpoch: 4 \tBatch: 150 \tLoss: 1.5004537105560303\nEpoch: 4 \tBatch: 151 \tLoss: 1.492401361465454\nEpoch: 4 \tBatch: 152 \tLoss: 1.4638973474502563\nEpoch: 4 \tBatch: 153 \tLoss: 1.4843125343322754\nEpoch: 4 \tBatch: 154 \tLoss: 1.481543779373169\nEpoch: 4 \tBatch: 155 \tLoss: 1.4692051410675049\nEpoch: 4 \tBatch: 156 \tLoss: 1.4726529121398926\nEpoch: 4 \tBatch: 157 \tLoss: 1.4769251346588135\nEpoch: 4 \tBatch: 158 \tLoss: 1.4981037378311157\nEpoch: 4 \tBatch: 159 \tLoss: 1.4729931354522705\nEpoch: 4 \tBatch: 160 \tLoss: 1.4803067445755005\nEpoch: 4 \tBatch: 161 \tLoss: 1.4788587093353271\nEpoch: 4 \tBatch: 162 \tLoss: 1.4952362775802612\nEpoch: 4 \tBatch: 163 \tLoss: 1.4756040573120117\nEpoch: 4 \tBatch: 164 \tLoss: 1.4781582355499268\nEpoch: 4 \tBatch: 165 \tLoss: 1.4857369661331177\nEpoch: 4 \tBatch: 166 \tLoss: 1.4886958599090576\nEpoch: 4 \tBatch: 167 \tLoss: 1.4794446229934692\nEpoch: 4 \tBatch: 168 \tLoss: 1.4725046157836914\nEpoch: 4 \tBatch: 169 \tLoss: 1.4798517227172852\nEpoch: 4 \tBatch: 170 \tLoss: 1.4783486127853394\nEpoch: 4 \tBatch: 171 \tLoss: 1.4660791158676147\nEpoch: 4 \tBatch: 172 \tLoss: 1.478758692741394\nEpoch: 4 \tBatch: 173 \tLoss: 1.4811437129974365\nEpoch: 4 \tBatch: 174 \tLoss: 1.4675713777542114\nEpoch: 4 \tBatch: 175 \tLoss: 1.470400094985962\nEpoch: 4 \tBatch: 176 \tLoss: 1.4729454517364502\nEpoch: 4 \tBatch: 177 \tLoss: 1.4986188411712646\nEpoch: 4 \tBatch: 178 \tLoss: 1.486670970916748\nEpoch: 4 \tBatch: 179 \tLoss: 1.4641376733779907\nEpoch: 4 \tBatch: 180 \tLoss: 1.477521538734436\nEpoch: 4 \tBatch: 181 \tLoss: 1.4919008016586304\nEpoch: 4 \tBatch: 182 \tLoss: 1.4725518226623535\nEpoch: 4 \tBatch: 183 \tLoss: 1.4668453931808472\nEpoch: 4 \tBatch: 184 \tLoss: 1.485201120376587\nEpoch: 4 \tBatch: 185 \tLoss: 1.482151985168457\nEpoch: 4 \tBatch: 186 \tLoss: 1.4902814626693726\nEpoch: 4 \tBatch: 187 \tLoss: 1.5014698505401611\nEpoch: 4 \tBatch: 188 \tLoss: 1.4863255023956299\nEpoch: 4 \tBatch: 189 \tLoss: 1.4636493921279907\nEpoch: 4 \tBatch: 190 \tLoss: 1.481175184249878\nEpoch: 4 \tBatch: 191 \tLoss: 1.4806009531021118\nEpoch: 4 \tBatch: 192 \tLoss: 1.4778650999069214\nEpoch: 4 \tBatch: 193 \tLoss: 1.4880934953689575\nEpoch: 4 \tBatch: 194 \tLoss: 1.4753910303115845\nEpoch: 4 \tBatch: 195 \tLoss: 1.496700644493103\nEpoch: 4 \tBatch: 196 \tLoss: 1.500996708869934\nEpoch: 4 \tBatch: 197 \tLoss: 1.471402645111084\nEpoch: 4 \tBatch: 198 \tLoss: 1.492171049118042\nEpoch: 4 \tBatch: 199 \tLoss: 1.471997857093811\nEpoch: 4 \tBatch: 200 \tLoss: 1.4643800258636475\nEpoch: 4 \tBatch: 201 \tLoss: 1.4756790399551392\nEpoch: 4 \tBatch: 202 \tLoss: 1.4810458421707153\nEpoch: 4 \tBatch: 203 \tLoss: 1.4698116779327393\nEpoch: 4 \tBatch: 204 \tLoss: 1.4837695360183716\nEpoch: 4 \tBatch: 205 \tLoss: 1.479643702507019\nEpoch: 4 \tBatch: 206 \tLoss: 1.4760924577713013\nEpoch: 4 \tBatch: 207 \tLoss: 1.4766291379928589\nEpoch: 4 \tBatch: 208 \tLoss: 1.4778586626052856\nEpoch: 4 \tBatch: 209 \tLoss: 1.4904597997665405\nEpoch: 4 \tBatch: 210 \tLoss: 1.470183253288269\nEpoch: 4 \tBatch: 211 \tLoss: 1.4862943887710571\nEpoch: 4 \tBatch: 212 \tLoss: 1.465038776397705\nEpoch: 4 \tBatch: 213 \tLoss: 1.4770300388336182\nEpoch: 4 \tBatch: 214 \tLoss: 1.4699209928512573\nEpoch: 4 \tBatch: 215 \tLoss: 1.4788891077041626\nEpoch: 4 \tBatch: 216 \tLoss: 1.475216269493103\nEpoch: 4 \tBatch: 217 \tLoss: 1.4830241203308105\nEpoch: 4 \tBatch: 218 \tLoss: 1.4616152048110962\nEpoch: 4 \tBatch: 219 \tLoss: 1.4707998037338257\nEpoch: 4 \tBatch: 220 \tLoss: 1.4782450199127197\nEpoch: 4 \tBatch: 221 \tLoss: 1.4753156900405884\nEpoch: 4 \tBatch: 222 \tLoss: 1.4863988161087036\nEpoch: 4 \tBatch: 223 \tLoss: 1.4793354272842407\nEpoch: 4 \tBatch: 224 \tLoss: 1.4899178743362427\nEpoch: 4 \tBatch: 225 \tLoss: 1.4719175100326538\nEpoch: 4 \tBatch: 226 \tLoss: 1.464342474937439\nEpoch: 4 \tBatch: 227 \tLoss: 1.4930986166000366\nEpoch: 4 \tBatch: 228 \tLoss: 1.4751451015472412\nEpoch: 4 \tBatch: 229 \tLoss: 1.4783434867858887\nEpoch: 4 \tBatch: 230 \tLoss: 1.4702876806259155\nEpoch: 4 \tBatch: 231 \tLoss: 1.469480037689209\nEpoch: 4 \tBatch: 232 \tLoss: 1.482862949371338\nEpoch: 4 \tBatch: 233 \tLoss: 1.4612832069396973\nEpoch: 4 \tBatch: 234 \tLoss: 1.481695294380188\nEpoch: 4 \tBatch: 235 \tLoss: 1.4797319173812866\nEpoch: 4 \tBatch: 236 \tLoss: 1.4823284149169922\nEpoch: 4 \tBatch: 237 \tLoss: 1.497331142425537\nEpoch: 4 \tBatch: 238 \tLoss: 1.4794963598251343\nEpoch: 4 \tBatch: 239 \tLoss: 1.486738681793213\nEpoch: 4 \tBatch: 240 \tLoss: 1.4826396703720093\nEpoch: 4 \tBatch: 241 \tLoss: 1.471451759338379\nEpoch: 4 \tBatch: 242 \tLoss: 1.4807192087173462\nEpoch: 4 \tBatch: 243 \tLoss: 1.4796297550201416\nEpoch: 4 \tBatch: 244 \tLoss: 1.4701308012008667\nEpoch: 4 \tBatch: 245 \tLoss: 1.4677560329437256\nEpoch: 4 \tBatch: 246 \tLoss: 1.477176547050476\nEpoch: 4 \tBatch: 247 \tLoss: 1.4730921983718872\nEpoch: 4 \tBatch: 248 \tLoss: 1.4871692657470703\nEpoch: 4 \tBatch: 249 \tLoss: 1.4689494371414185\nEpoch: 4 \tBatch: 250 \tLoss: 1.4712893962860107\nEpoch: 4 \tBatch: 251 \tLoss: 1.4964519739151\nEpoch: 4 \tBatch: 252 \tLoss: 1.4715752601623535\nEpoch: 4 \tBatch: 253 \tLoss: 1.4699440002441406\nEpoch: 4 \tBatch: 254 \tLoss: 1.4796191453933716\nEpoch: 4 \tBatch: 255 \tLoss: 1.472253680229187\nEpoch: 4 \tBatch: 256 \tLoss: 1.4771928787231445\nEpoch: 4 \tBatch: 257 \tLoss: 1.4697247743606567\nEpoch: 4 \tBatch: 258 \tLoss: 1.4893200397491455\nEpoch: 4 \tBatch: 259 \tLoss: 1.477786898612976\nEpoch: 4 \tBatch: 260 \tLoss: 1.4808294773101807\nEpoch: 4 \tBatch: 261 \tLoss: 1.4702454805374146\nEpoch: 4 \tBatch: 262 \tLoss: 1.4618070125579834\nEpoch: 4 \tBatch: 263 \tLoss: 1.484855055809021\nEpoch: 4 \tBatch: 264 \tLoss: 1.4699229001998901\nEpoch: 4 \tBatch: 265 \tLoss: 1.4874179363250732\nEpoch: 4 \tBatch: 266 \tLoss: 1.4701967239379883\nEpoch: 4 \tBatch: 267 \tLoss: 1.4783543348312378\nEpoch: 4 \tBatch: 268 \tLoss: 1.4726157188415527\nEpoch: 4 \tBatch: 269 \tLoss: 1.5056341886520386\nEpoch: 4 \tBatch: 270 \tLoss: 1.477442979812622\nEpoch: 4 \tBatch: 271 \tLoss: 1.469986915588379\nEpoch: 4 \tBatch: 272 \tLoss: 1.4739004373550415\nEpoch: 4 \tBatch: 273 \tLoss: 1.4841375350952148\nEpoch: 4 \tBatch: 274 \tLoss: 1.477997064590454\nEpoch: 4 \tBatch: 275 \tLoss: 1.4869450330734253\nEpoch: 4 \tBatch: 276 \tLoss: 1.478094458580017\nEpoch: 4 \tBatch: 277 \tLoss: 1.500109314918518\nEpoch: 4 \tBatch: 278 \tLoss: 1.4773032665252686\nEpoch: 4 \tBatch: 279 \tLoss: 1.4619250297546387\nEpoch: 4 \tBatch: 280 \tLoss: 1.4648420810699463\nEpoch: 4 \tBatch: 281 \tLoss: 1.4900202751159668\nEpoch: 4 \tBatch: 282 \tLoss: 1.469797968864441\nEpoch: 4 \tBatch: 283 \tLoss: 1.468603491783142\nEpoch: 4 \tBatch: 284 \tLoss: 1.4976633787155151\nEpoch: 4 \tBatch: 285 \tLoss: 1.4624420404434204\nEpoch: 4 \tBatch: 286 \tLoss: 1.472065806388855\nEpoch: 4 \tBatch: 287 \tLoss: 1.4802459478378296\nEpoch: 4 \tBatch: 288 \tLoss: 1.4832741022109985\nEpoch: 4 \tBatch: 289 \tLoss: 1.4857808351516724\nEpoch: 4 \tBatch: 290 \tLoss: 1.483939528465271\nEpoch: 4 \tBatch: 291 \tLoss: 1.4683492183685303\nEpoch: 4 \tBatch: 292 \tLoss: 1.4698349237442017\nEpoch: 4 \tBatch: 293 \tLoss: 1.4760044813156128\nEpoch: 4 \tBatch: 294 \tLoss: 1.4731065034866333\nEpoch: 4 \tBatch: 295 \tLoss: 1.4682878255844116\nEpoch: 4 \tBatch: 296 \tLoss: 1.4649126529693604\nEpoch: 4 \tBatch: 297 \tLoss: 1.4702560901641846\nEpoch: 4 \tBatch: 298 \tLoss: 1.4904996156692505\nEpoch: 4 \tBatch: 299 \tLoss: 1.5014293193817139\nEpoch: 4 \tBatch: 300 \tLoss: 1.4906686544418335\nEpoch: 4 \tBatch: 301 \tLoss: 1.4910808801651\nEpoch: 4 \tBatch: 302 \tLoss: 1.487231731414795\nEpoch: 4 \tBatch: 303 \tLoss: 1.4755765199661255\nEpoch: 4 \tBatch: 304 \tLoss: 1.4785317182540894\nEpoch: 4 \tBatch: 305 \tLoss: 1.475743293762207\nEpoch: 4 \tBatch: 306 \tLoss: 1.49003005027771\nEpoch: 4 \tBatch: 307 \tLoss: 1.4927483797073364\nEpoch: 4 \tBatch: 308 \tLoss: 1.4820574522018433\nEpoch: 4 \tBatch: 309 \tLoss: 1.4910742044448853\nEpoch: 4 \tBatch: 310 \tLoss: 1.4646575450897217\nEpoch: 4 \tBatch: 311 \tLoss: 1.4800113439559937\nEpoch: 4 \tBatch: 312 \tLoss: 1.4716084003448486\nEpoch: 4 \tBatch: 313 \tLoss: 1.47919762134552\nEpoch: 4 \tBatch: 314 \tLoss: 1.4818637371063232\nEpoch: 4 \tBatch: 315 \tLoss: 1.4918591976165771\nEpoch: 4 \tBatch: 316 \tLoss: 1.496756672859192\nEpoch: 4 \tBatch: 317 \tLoss: 1.4769320487976074\nEpoch: 4 \tBatch: 318 \tLoss: 1.4833152294158936\nEpoch: 4 \tBatch: 319 \tLoss: 1.4941407442092896\nEpoch: 4 \tBatch: 320 \tLoss: 1.4699193239212036\nEpoch: 4 \tBatch: 321 \tLoss: 1.4832432270050049\nEpoch: 4 \tBatch: 322 \tLoss: 1.477238655090332\nEpoch: 4 \tBatch: 323 \tLoss: 1.4750010967254639\nEpoch: 4 \tBatch: 324 \tLoss: 1.4870818853378296\nEpoch: 4 \tBatch: 325 \tLoss: 1.4701327085494995\nEpoch: 4 \tBatch: 326 \tLoss: 1.5023126602172852\nEpoch: 4 \tBatch: 327 \tLoss: 1.475041389465332\nEpoch: 4 \tBatch: 328 \tLoss: 1.4755538702011108\nEpoch: 4 \tBatch: 329 \tLoss: 1.4822773933410645\nEpoch: 4 \tBatch: 330 \tLoss: 1.5034997463226318\nEpoch: 4 \tBatch: 331 \tLoss: 1.479085922241211\nEpoch: 4 \tBatch: 332 \tLoss: 1.4808720350265503\nEpoch: 4 \tBatch: 333 \tLoss: 1.4796630144119263\nEpoch: 4 \tBatch: 334 \tLoss: 1.489518404006958\nEpoch: 4 \tBatch: 335 \tLoss: 1.4726113080978394\nEpoch: 4 \tBatch: 336 \tLoss: 1.485469102859497\nEpoch: 4 \tBatch: 337 \tLoss: 1.4743094444274902\nEpoch: 4 \tBatch: 338 \tLoss: 1.4840251207351685\nEpoch: 4 \tBatch: 339 \tLoss: 1.514119029045105\nEpoch: 4 \tBatch: 340 \tLoss: 1.4832792282104492\nEpoch: 4 \tBatch: 341 \tLoss: 1.4619609117507935\nEpoch: 4 \tBatch: 342 \tLoss: 1.4768049716949463\nEpoch: 4 \tBatch: 343 \tLoss: 1.4631599187850952\nEpoch: 4 \tBatch: 344 \tLoss: 1.4734818935394287\nEpoch: 4 \tBatch: 345 \tLoss: 1.4742621183395386\nEpoch: 4 \tBatch: 346 \tLoss: 1.4800294637680054\nEpoch: 4 \tBatch: 347 \tLoss: 1.492570161819458\nEpoch: 4 \tBatch: 348 \tLoss: 1.4799178838729858\nEpoch: 4 \tBatch: 349 \tLoss: 1.470324158668518\nEpoch: 4 \tBatch: 350 \tLoss: 1.4714243412017822\nEpoch: 4 \tBatch: 351 \tLoss: 1.477587342262268\nEpoch: 4 \tBatch: 352 \tLoss: 1.4920129776000977\nEpoch: 4 \tBatch: 353 \tLoss: 1.4944401979446411\nEpoch: 4 \tBatch: 354 \tLoss: 1.4767755270004272\nEpoch: 4 \tBatch: 355 \tLoss: 1.4737576246261597\nEpoch: 4 \tBatch: 356 \tLoss: 1.4979974031448364\nEpoch: 4 \tBatch: 357 \tLoss: 1.471489429473877\nEpoch: 4 \tBatch: 358 \tLoss: 1.48371160030365\nEpoch: 4 \tBatch: 359 \tLoss: 1.4774965047836304\nEpoch: 4 \tBatch: 360 \tLoss: 1.4636108875274658\nEpoch: 4 \tBatch: 361 \tLoss: 1.4716037511825562\nEpoch: 4 \tBatch: 362 \tLoss: 1.498110294342041\nEpoch: 4 \tBatch: 363 \tLoss: 1.4794427156448364\nEpoch: 4 \tBatch: 364 \tLoss: 1.4836206436157227\nEpoch: 4 \tBatch: 365 \tLoss: 1.4849570989608765\nEpoch: 4 \tBatch: 366 \tLoss: 1.466896653175354\nEpoch: 4 \tBatch: 367 \tLoss: 1.4866055250167847\nEpoch: 4 \tBatch: 368 \tLoss: 1.4647035598754883\nEpoch: 4 \tBatch: 369 \tLoss: 1.4791001081466675\nEpoch: 4 \tBatch: 370 \tLoss: 1.4759066104888916\nEpoch: 4 \tBatch: 371 \tLoss: 1.4756789207458496\nEpoch: 4 \tBatch: 372 \tLoss: 1.4843366146087646\nEpoch: 4 \tBatch: 373 \tLoss: 1.4867662191390991\nEpoch: 4 \tBatch: 374 \tLoss: 1.477831482887268\nEpoch: 4 \tBatch: 375 \tLoss: 1.4718209505081177\nEpoch: 4 \tBatch: 376 \tLoss: 1.4703038930892944\nEpoch: 4 \tBatch: 377 \tLoss: 1.500519037246704\nEpoch: 4 \tBatch: 378 \tLoss: 1.4696569442749023\nEpoch: 4 \tBatch: 379 \tLoss: 1.4949043989181519\nEpoch: 4 \tBatch: 380 \tLoss: 1.4716135263442993\nEpoch: 4 \tBatch: 381 \tLoss: 1.4798860549926758\nEpoch: 4 \tBatch: 382 \tLoss: 1.4765599966049194\nEpoch: 4 \tBatch: 383 \tLoss: 1.4755804538726807\nEpoch: 4 \tBatch: 384 \tLoss: 1.4825944900512695\nEpoch: 4 \tBatch: 385 \tLoss: 1.4766457080841064\nEpoch: 4 \tBatch: 386 \tLoss: 1.4883297681808472\nEpoch: 4 \tBatch: 387 \tLoss: 1.4633421897888184\nEpoch: 4 \tBatch: 388 \tLoss: 1.4856455326080322\nEpoch: 4 \tBatch: 389 \tLoss: 1.4714586734771729\nEpoch: 4 \tBatch: 390 \tLoss: 1.5058127641677856\nTraining Complete. Final loss = 1.5058127641677856\n"
    }
   ],
   "source": [
    "def train(model, epochs, writer, verbose=True, tag='Loss/Train'):\n",
    "    for epoch in range(epochs):\n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            # pass x through your model to get a prediction\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the cost\n",
    "            if verbose: print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss.item())\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of all of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and store all of the model's param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            \n",
    "            writer.add_scalar(tag, loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "    print('Training Complete. Final loss =',loss.item())\n",
    "    \n",
    "train(cnn, epochs, writer=writer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train Accuracy: 98.652\nValidation Accuracy: 98.19\nTest Accuracy: 98.31\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def calc_accuracy(model, dataloader):\n",
    "    num_correct = 0\n",
    "    num_examples = len(dataloader.dataset)                       # test DATA not test LOADER\n",
    "    for inputs, labels in dataloader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    return percent_correct\n",
    "\n",
    "print('Train Accuracy:', calc_accuracy(cnn, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn, test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Compare to cnn without batch normalisation <br>\n",
    "Use a larger lr with batch normalisation\n",
    "Add plots to show stability and fast convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "4 \tLoss: 1.4690825939178467\nEpoch: 3 \tBatch: 355 \tLoss: 1.504518985748291\nEpoch: 3 \tBatch: 356 \tLoss: 1.473148226737976\nEpoch: 3 \tBatch: 357 \tLoss: 1.4720790386199951\nEpoch: 3 \tBatch: 358 \tLoss: 1.4752596616744995\nEpoch: 3 \tBatch: 359 \tLoss: 1.462831735610962\nEpoch: 3 \tBatch: 360 \tLoss: 1.4618823528289795\nEpoch: 3 \tBatch: 361 \tLoss: 1.5017436742782593\nEpoch: 3 \tBatch: 362 \tLoss: 1.4638078212738037\nEpoch: 3 \tBatch: 363 \tLoss: 1.4766042232513428\nEpoch: 3 \tBatch: 364 \tLoss: 1.462192177772522\nEpoch: 3 \tBatch: 365 \tLoss: 1.4612401723861694\nEpoch: 3 \tBatch: 366 \tLoss: 1.4644782543182373\nEpoch: 3 \tBatch: 367 \tLoss: 1.4747031927108765\nEpoch: 3 \tBatch: 368 \tLoss: 1.4773313999176025\nEpoch: 3 \tBatch: 369 \tLoss: 1.4613133668899536\nEpoch: 3 \tBatch: 370 \tLoss: 1.46896231174469\nEpoch: 3 \tBatch: 371 \tLoss: 1.4839258193969727\nEpoch: 3 \tBatch: 372 \tLoss: 1.4717891216278076\nEpoch: 3 \tBatch: 373 \tLoss: 1.4728102684020996\nEpoch: 3 \tBatch: 374 \tLoss: 1.4613990783691406\nEpoch: 3 \tBatch: 375 \tLoss: 1.4664561748504639\nEpoch: 3 \tBatch: 376 \tLoss: 1.4691585302352905\nEpoch: 3 \tBatch: 377 \tLoss: 1.4716354608535767\nEpoch: 3 \tBatch: 378 \tLoss: 1.4768370389938354\nEpoch: 3 \tBatch: 379 \tLoss: 1.4704915285110474\nEpoch: 3 \tBatch: 380 \tLoss: 1.5089614391326904\nEpoch: 3 \tBatch: 381 \tLoss: 1.465684413909912\nEpoch: 3 \tBatch: 382 \tLoss: 1.4723973274230957\nEpoch: 3 \tBatch: 383 \tLoss: 1.4773693084716797\nEpoch: 3 \tBatch: 384 \tLoss: 1.4753626585006714\nEpoch: 3 \tBatch: 385 \tLoss: 1.467221736907959\nEpoch: 3 \tBatch: 386 \tLoss: 1.473514199256897\nEpoch: 3 \tBatch: 387 \tLoss: 1.4691200256347656\nEpoch: 3 \tBatch: 388 \tLoss: 1.4808262586593628\nEpoch: 3 \tBatch: 389 \tLoss: 1.4661245346069336\nEpoch: 3 \tBatch: 390 \tLoss: 1.4736868143081665\nEpoch: 4 \tBatch: 0 \tLoss: 1.4699130058288574\nEpoch: 4 \tBatch: 1 \tLoss: 1.4838626384735107\nEpoch: 4 \tBatch: 2 \tLoss: 1.471688985824585\nEpoch: 4 \tBatch: 3 \tLoss: 1.4676172733306885\nEpoch: 4 \tBatch: 4 \tLoss: 1.4636610746383667\nEpoch: 4 \tBatch: 5 \tLoss: 1.478004813194275\nEpoch: 4 \tBatch: 6 \tLoss: 1.4711556434631348\nEpoch: 4 \tBatch: 7 \tLoss: 1.4645925760269165\nEpoch: 4 \tBatch: 8 \tLoss: 1.4789557456970215\nEpoch: 4 \tBatch: 9 \tLoss: 1.4751670360565186\nEpoch: 4 \tBatch: 10 \tLoss: 1.4643656015396118\nEpoch: 4 \tBatch: 11 \tLoss: 1.480182409286499\nEpoch: 4 \tBatch: 12 \tLoss: 1.4676566123962402\nEpoch: 4 \tBatch: 13 \tLoss: 1.4840803146362305\nEpoch: 4 \tBatch: 14 \tLoss: 1.461391806602478\nEpoch: 4 \tBatch: 15 \tLoss: 1.4685808420181274\nEpoch: 4 \tBatch: 16 \tLoss: 1.4934583902359009\nEpoch: 4 \tBatch: 17 \tLoss: 1.4685758352279663\nEpoch: 4 \tBatch: 18 \tLoss: 1.4744129180908203\nEpoch: 4 \tBatch: 19 \tLoss: 1.4769973754882812\nEpoch: 4 \tBatch: 20 \tLoss: 1.482743740081787\nEpoch: 4 \tBatch: 21 \tLoss: 1.469278335571289\nEpoch: 4 \tBatch: 22 \tLoss: 1.4761624336242676\nEpoch: 4 \tBatch: 23 \tLoss: 1.4675010442733765\nEpoch: 4 \tBatch: 24 \tLoss: 1.4674922227859497\nEpoch: 4 \tBatch: 25 \tLoss: 1.4619505405426025\nEpoch: 4 \tBatch: 26 \tLoss: 1.46193528175354\nEpoch: 4 \tBatch: 27 \tLoss: 1.4777666330337524\nEpoch: 4 \tBatch: 28 \tLoss: 1.4804751873016357\nEpoch: 4 \tBatch: 29 \tLoss: 1.4653027057647705\nEpoch: 4 \tBatch: 30 \tLoss: 1.4633760452270508\nEpoch: 4 \tBatch: 31 \tLoss: 1.4680819511413574\nEpoch: 4 \tBatch: 32 \tLoss: 1.4689069986343384\nEpoch: 4 \tBatch: 33 \tLoss: 1.4657325744628906\nEpoch: 4 \tBatch: 34 \tLoss: 1.4644356966018677\nEpoch: 4 \tBatch: 35 \tLoss: 1.466165542602539\nEpoch: 4 \tBatch: 36 \tLoss: 1.4906924962997437\nEpoch: 4 \tBatch: 37 \tLoss: 1.4939953088760376\nEpoch: 4 \tBatch: 38 \tLoss: 1.4694846868515015\nEpoch: 4 \tBatch: 39 \tLoss: 1.4646704196929932\nEpoch: 4 \tBatch: 40 \tLoss: 1.475852608680725\nEpoch: 4 \tBatch: 41 \tLoss: 1.4689927101135254\nEpoch: 4 \tBatch: 42 \tLoss: 1.4795068502426147\nEpoch: 4 \tBatch: 43 \tLoss: 1.4695954322814941\nEpoch: 4 \tBatch: 44 \tLoss: 1.4615166187286377\nEpoch: 4 \tBatch: 45 \tLoss: 1.495262622833252\nEpoch: 4 \tBatch: 46 \tLoss: 1.4785284996032715\nEpoch: 4 \tBatch: 47 \tLoss: 1.4700078964233398\nEpoch: 4 \tBatch: 48 \tLoss: 1.4881207942962646\nEpoch: 4 \tBatch: 49 \tLoss: 1.4686962366104126\nEpoch: 4 \tBatch: 50 \tLoss: 1.4697456359863281\nEpoch: 4 \tBatch: 51 \tLoss: 1.461236596107483\nEpoch: 4 \tBatch: 52 \tLoss: 1.4612071514129639\nEpoch: 4 \tBatch: 53 \tLoss: 1.4611644744873047\nEpoch: 4 \tBatch: 54 \tLoss: 1.479928731918335\nEpoch: 4 \tBatch: 55 \tLoss: 1.4793695211410522\nEpoch: 4 \tBatch: 56 \tLoss: 1.4673962593078613\nEpoch: 4 \tBatch: 57 \tLoss: 1.4690046310424805\nEpoch: 4 \tBatch: 58 \tLoss: 1.4699926376342773\nEpoch: 4 \tBatch: 59 \tLoss: 1.4767377376556396\nEpoch: 4 \tBatch: 60 \tLoss: 1.4774693250656128\nEpoch: 4 \tBatch: 61 \tLoss: 1.477221965789795\nEpoch: 4 \tBatch: 62 \tLoss: 1.4812953472137451\nEpoch: 4 \tBatch: 63 \tLoss: 1.4627491235733032\nEpoch: 4 \tBatch: 64 \tLoss: 1.4613680839538574\nEpoch: 4 \tBatch: 65 \tLoss: 1.4686574935913086\nEpoch: 4 \tBatch: 66 \tLoss: 1.4768086671829224\nEpoch: 4 \tBatch: 67 \tLoss: 1.4785363674163818\nEpoch: 4 \tBatch: 68 \tLoss: 1.4613168239593506\nEpoch: 4 \tBatch: 69 \tLoss: 1.4685108661651611\nEpoch: 4 \tBatch: 70 \tLoss: 1.4694249629974365\nEpoch: 4 \tBatch: 71 \tLoss: 1.4728163480758667\nEpoch: 4 \tBatch: 72 \tLoss: 1.46309232711792\nEpoch: 4 \tBatch: 73 \tLoss: 1.4692015647888184\nEpoch: 4 \tBatch: 74 \tLoss: 1.479949712753296\nEpoch: 4 \tBatch: 75 \tLoss: 1.469700574874878\nEpoch: 4 \tBatch: 76 \tLoss: 1.4612596035003662\nEpoch: 4 \tBatch: 77 \tLoss: 1.4793035984039307\nEpoch: 4 \tBatch: 78 \tLoss: 1.4693228006362915\nEpoch: 4 \tBatch: 79 \tLoss: 1.4792675971984863\nEpoch: 4 \tBatch: 80 \tLoss: 1.4849215745925903\nEpoch: 4 \tBatch: 81 \tLoss: 1.4682481288909912\nEpoch: 4 \tBatch: 82 \tLoss: 1.4690018892288208\nEpoch: 4 \tBatch: 83 \tLoss: 1.4690673351287842\nEpoch: 4 \tBatch: 84 \tLoss: 1.4745638370513916\nEpoch: 4 \tBatch: 85 \tLoss: 1.47182297706604\nEpoch: 4 \tBatch: 86 \tLoss: 1.4708086252212524\nEpoch: 4 \tBatch: 87 \tLoss: 1.4619659185409546\nEpoch: 4 \tBatch: 88 \tLoss: 1.4774954319000244\nEpoch: 4 \tBatch: 89 \tLoss: 1.4803802967071533\nEpoch: 4 \tBatch: 90 \tLoss: 1.483511209487915\nEpoch: 4 \tBatch: 91 \tLoss: 1.4859808683395386\nEpoch: 4 \tBatch: 92 \tLoss: 1.4698293209075928\nEpoch: 4 \tBatch: 93 \tLoss: 1.4761143922805786\nEpoch: 4 \tBatch: 94 \tLoss: 1.4689388275146484\nEpoch: 4 \tBatch: 95 \tLoss: 1.4627180099487305\nEpoch: 4 \tBatch: 96 \tLoss: 1.4695606231689453\nEpoch: 4 \tBatch: 97 \tLoss: 1.4616076946258545\nEpoch: 4 \tBatch: 98 \tLoss: 1.469222068786621\nEpoch: 4 \tBatch: 99 \tLoss: 1.4659030437469482\nEpoch: 4 \tBatch: 100 \tLoss: 1.4774811267852783\nEpoch: 4 \tBatch: 101 \tLoss: 1.462805151939392\nEpoch: 4 \tBatch: 102 \tLoss: 1.5010868310928345\nEpoch: 4 \tBatch: 103 \tLoss: 1.4621175527572632\nEpoch: 4 \tBatch: 104 \tLoss: 1.4704639911651611\nEpoch: 4 \tBatch: 105 \tLoss: 1.477354645729065\nEpoch: 4 \tBatch: 106 \tLoss: 1.4693632125854492\nEpoch: 4 \tBatch: 107 \tLoss: 1.4661622047424316\nEpoch: 4 \tBatch: 108 \tLoss: 1.4770097732543945\nEpoch: 4 \tBatch: 109 \tLoss: 1.4723104238510132\nEpoch: 4 \tBatch: 110 \tLoss: 1.4674592018127441\nEpoch: 4 \tBatch: 111 \tLoss: 1.46139395236969\nEpoch: 4 \tBatch: 112 \tLoss: 1.4811064004898071\nEpoch: 4 \tBatch: 113 \tLoss: 1.486756443977356\nEpoch: 4 \tBatch: 114 \tLoss: 1.467606782913208\nEpoch: 4 \tBatch: 115 \tLoss: 1.4676512479782104\nEpoch: 4 \tBatch: 116 \tLoss: 1.4773355722427368\nEpoch: 4 \tBatch: 117 \tLoss: 1.4760768413543701\nEpoch: 4 \tBatch: 118 \tLoss: 1.4775700569152832\nEpoch: 4 \tBatch: 119 \tLoss: 1.4771324396133423\nEpoch: 4 \tBatch: 120 \tLoss: 1.4685113430023193\nEpoch: 4 \tBatch: 121 \tLoss: 1.468449592590332\nEpoch: 4 \tBatch: 122 \tLoss: 1.4780380725860596\nEpoch: 4 \tBatch: 123 \tLoss: 1.489159107208252\nEpoch: 4 \tBatch: 124 \tLoss: 1.4674110412597656\nEpoch: 4 \tBatch: 125 \tLoss: 1.4842530488967896\nEpoch: 4 \tBatch: 126 \tLoss: 1.471152663230896\nEpoch: 4 \tBatch: 127 \tLoss: 1.469123363494873\nEpoch: 4 \tBatch: 128 \tLoss: 1.4997090101242065\nEpoch: 4 \tBatch: 129 \tLoss: 1.4694700241088867\nEpoch: 4 \tBatch: 130 \tLoss: 1.472432255744934\nEpoch: 4 \tBatch: 131 \tLoss: 1.4623942375183105\nEpoch: 4 \tBatch: 132 \tLoss: 1.4697339534759521\nEpoch: 4 \tBatch: 133 \tLoss: 1.461787462234497\nEpoch: 4 \tBatch: 134 \tLoss: 1.4769164323806763\nEpoch: 4 \tBatch: 135 \tLoss: 1.461242914199829\nEpoch: 4 \tBatch: 136 \tLoss: 1.470884919166565\nEpoch: 4 \tBatch: 137 \tLoss: 1.4617632627487183\nEpoch: 4 \tBatch: 138 \tLoss: 1.4739816188812256\nEpoch: 4 \tBatch: 139 \tLoss: 1.4773482084274292\nEpoch: 4 \tBatch: 140 \tLoss: 1.4779232740402222\nEpoch: 4 \tBatch: 141 \tLoss: 1.47685968875885\nEpoch: 4 \tBatch: 142 \tLoss: 1.4892579317092896\nEpoch: 4 \tBatch: 143 \tLoss: 1.4767873287200928\nEpoch: 4 \tBatch: 144 \tLoss: 1.4769827127456665\nEpoch: 4 \tBatch: 145 \tLoss: 1.4620029926300049\nEpoch: 4 \tBatch: 146 \tLoss: 1.4622459411621094\nEpoch: 4 \tBatch: 147 \tLoss: 1.4621855020523071\nEpoch: 4 \tBatch: 148 \tLoss: 1.4611531496047974\nEpoch: 4 \tBatch: 149 \tLoss: 1.4842525720596313\nEpoch: 4 \tBatch: 150 \tLoss: 1.48173987865448\nEpoch: 4 \tBatch: 151 \tLoss: 1.473522424697876\nEpoch: 4 \tBatch: 152 \tLoss: 1.464881420135498\nEpoch: 4 \tBatch: 153 \tLoss: 1.4684418439865112\nEpoch: 4 \tBatch: 154 \tLoss: 1.48698890209198\nEpoch: 4 \tBatch: 155 \tLoss: 1.4890031814575195\nEpoch: 4 \tBatch: 156 \tLoss: 1.4691966772079468\nEpoch: 4 \tBatch: 157 \tLoss: 1.4693423509597778\nEpoch: 4 \tBatch: 158 \tLoss: 1.492142677307129\nEpoch: 4 \tBatch: 159 \tLoss: 1.4642558097839355\nEpoch: 4 \tBatch: 160 \tLoss: 1.4643359184265137\nEpoch: 4 \tBatch: 161 \tLoss: 1.4702855348587036\nEpoch: 4 \tBatch: 162 \tLoss: 1.4722546339035034\nEpoch: 4 \tBatch: 163 \tLoss: 1.4688868522644043\nEpoch: 4 \tBatch: 164 \tLoss: 1.4613252878189087\nEpoch: 4 \tBatch: 165 \tLoss: 1.4695382118225098\nEpoch: 4 \tBatch: 166 \tLoss: 1.497280240058899\nEpoch: 4 \tBatch: 167 \tLoss: 1.4615024328231812\nEpoch: 4 \tBatch: 168 \tLoss: 1.4612292051315308\nEpoch: 4 \tBatch: 169 \tLoss: 1.4715615510940552\nEpoch: 4 \tBatch: 170 \tLoss: 1.4767059087753296\nEpoch: 4 \tBatch: 171 \tLoss: 1.4694799184799194\nEpoch: 4 \tBatch: 172 \tLoss: 1.465155005455017\nEpoch: 4 \tBatch: 173 \tLoss: 1.465867280960083\nEpoch: 4 \tBatch: 174 \tLoss: 1.4705058336257935\nEpoch: 4 \tBatch: 175 \tLoss: 1.4690788984298706\nEpoch: 4 \tBatch: 176 \tLoss: 1.4613240957260132\nEpoch: 4 \tBatch: 177 \tLoss: 1.4841556549072266\nEpoch: 4 \tBatch: 178 \tLoss: 1.4644156694412231\nEpoch: 4 \tBatch: 179 \tLoss: 1.4721194505691528\nEpoch: 4 \tBatch: 180 \tLoss: 1.47622549533844\nEpoch: 4 \tBatch: 181 \tLoss: 1.4690901041030884\nEpoch: 4 \tBatch: 182 \tLoss: 1.4619232416152954\nEpoch: 4 \tBatch: 183 \tLoss: 1.4691555500030518\nEpoch: 4 \tBatch: 184 \tLoss: 1.4945006370544434\nEpoch: 4 \tBatch: 185 \tLoss: 1.4787330627441406\nEpoch: 4 \tBatch: 186 \tLoss: 1.462217092514038\nEpoch: 4 \tBatch: 187 \tLoss: 1.4761205911636353\nEpoch: 4 \tBatch: 188 \tLoss: 1.4632130861282349\nEpoch: 4 \tBatch: 189 \tLoss: 1.4611743688583374\nEpoch: 4 \tBatch: 190 \tLoss: 1.4765255451202393\nEpoch: 4 \tBatch: 191 \tLoss: 1.4613041877746582\nEpoch: 4 \tBatch: 192 \tLoss: 1.4923369884490967\nEpoch: 4 \tBatch: 193 \tLoss: 1.4665378332138062\nEpoch: 4 \tBatch: 194 \tLoss: 1.4689675569534302\nEpoch: 4 \tBatch: 195 \tLoss: 1.4717352390289307\nEpoch: 4 \tBatch: 196 \tLoss: 1.468907117843628\nEpoch: 4 \tBatch: 197 \tLoss: 1.4611769914627075\nEpoch: 4 \tBatch: 198 \tLoss: 1.4611512422561646\nEpoch: 4 \tBatch: 199 \tLoss: 1.4613571166992188\nEpoch: 4 \tBatch: 200 \tLoss: 1.461219310760498\nEpoch: 4 \tBatch: 201 \tLoss: 1.4762026071548462\nEpoch: 4 \tBatch: 202 \tLoss: 1.469177007675171\nEpoch: 4 \tBatch: 203 \tLoss: 1.4667623043060303\nEpoch: 4 \tBatch: 204 \tLoss: 1.4690934419631958\nEpoch: 4 \tBatch: 205 \tLoss: 1.4689911603927612\nEpoch: 4 \tBatch: 206 \tLoss: 1.4615137577056885\nEpoch: 4 \tBatch: 207 \tLoss: 1.4892746210098267\nEpoch: 4 \tBatch: 208 \tLoss: 1.4691786766052246\nEpoch: 4 \tBatch: 209 \tLoss: 1.469356656074524\nEpoch: 4 \tBatch: 210 \tLoss: 1.4615710973739624\nEpoch: 4 \tBatch: 211 \tLoss: 1.4689650535583496\nEpoch: 4 \tBatch: 212 \tLoss: 1.467437982559204\nEpoch: 4 \tBatch: 213 \tLoss: 1.4691240787506104\nEpoch: 4 \tBatch: 214 \tLoss: 1.461156964302063\nEpoch: 4 \tBatch: 215 \tLoss: 1.4736698865890503\nEpoch: 4 \tBatch: 216 \tLoss: 1.4846349954605103\nEpoch: 4 \tBatch: 217 \tLoss: 1.4617176055908203\nEpoch: 4 \tBatch: 218 \tLoss: 1.4627619981765747\nEpoch: 4 \tBatch: 219 \tLoss: 1.4621728658676147\nEpoch: 4 \tBatch: 220 \tLoss: 1.4618375301361084\nEpoch: 4 \tBatch: 221 \tLoss: 1.4759891033172607\nEpoch: 4 \tBatch: 222 \tLoss: 1.4689202308654785\nEpoch: 4 \tBatch: 223 \tLoss: 1.4676947593688965\nEpoch: 4 \tBatch: 224 \tLoss: 1.4690988063812256\nEpoch: 4 \tBatch: 225 \tLoss: 1.47231924533844\nEpoch: 4 \tBatch: 226 \tLoss: 1.4620475769042969\nEpoch: 4 \tBatch: 227 \tLoss: 1.4953114986419678\nEpoch: 4 \tBatch: 228 \tLoss: 1.4675370454788208\nEpoch: 4 \tBatch: 229 \tLoss: 1.461158275604248\nEpoch: 4 \tBatch: 230 \tLoss: 1.4611520767211914\nEpoch: 4 \tBatch: 231 \tLoss: 1.463156819343567\nEpoch: 4 \tBatch: 232 \tLoss: 1.476783275604248\nEpoch: 4 \tBatch: 233 \tLoss: 1.4612035751342773\nEpoch: 4 \tBatch: 234 \tLoss: 1.4690333604812622\nEpoch: 4 \tBatch: 235 \tLoss: 1.4799418449401855\nEpoch: 4 \tBatch: 236 \tLoss: 1.4643752574920654\nEpoch: 4 \tBatch: 237 \tLoss: 1.4639919996261597\nEpoch: 4 \tBatch: 238 \tLoss: 1.461279034614563\nEpoch: 4 \tBatch: 239 \tLoss: 1.468970775604248\nEpoch: 4 \tBatch: 240 \tLoss: 1.461498498916626\nEpoch: 4 \tBatch: 241 \tLoss: 1.4665480852127075\nEpoch: 4 \tBatch: 242 \tLoss: 1.4767025709152222\nEpoch: 4 \tBatch: 243 \tLoss: 1.4691259860992432\nEpoch: 4 \tBatch: 244 \tLoss: 1.4733442068099976\nEpoch: 4 \tBatch: 245 \tLoss: 1.4773855209350586\nEpoch: 4 \tBatch: 246 \tLoss: 1.4624037742614746\nEpoch: 4 \tBatch: 247 \tLoss: 1.4636276960372925\nEpoch: 4 \tBatch: 248 \tLoss: 1.4772310256958008\nEpoch: 4 \tBatch: 249 \tLoss: 1.4634255170822144\nEpoch: 4 \tBatch: 250 \tLoss: 1.4809635877609253\nEpoch: 4 \tBatch: 251 \tLoss: 1.4692387580871582\nEpoch: 4 \tBatch: 252 \tLoss: 1.4799522161483765\nEpoch: 4 \tBatch: 253 \tLoss: 1.4685442447662354\nEpoch: 4 \tBatch: 254 \tLoss: 1.4681220054626465\nEpoch: 4 \tBatch: 255 \tLoss: 1.483364462852478\nEpoch: 4 \tBatch: 256 \tLoss: 1.467991828918457\nEpoch: 4 \tBatch: 257 \tLoss: 1.469150424003601\nEpoch: 4 \tBatch: 258 \tLoss: 1.4843155145645142\nEpoch: 4 \tBatch: 259 \tLoss: 1.470763087272644\nEpoch: 4 \tBatch: 260 \tLoss: 1.471771001815796\nEpoch: 4 \tBatch: 261 \tLoss: 1.4611790180206299\nEpoch: 4 \tBatch: 262 \tLoss: 1.4842395782470703\nEpoch: 4 \tBatch: 263 \tLoss: 1.4829992055892944\nEpoch: 4 \tBatch: 264 \tLoss: 1.47947096824646\nEpoch: 4 \tBatch: 265 \tLoss: 1.482795000076294\nEpoch: 4 \tBatch: 266 \tLoss: 1.4847396612167358\nEpoch: 4 \tBatch: 267 \tLoss: 1.4613432884216309\nEpoch: 4 \tBatch: 268 \tLoss: 1.4664666652679443\nEpoch: 4 \tBatch: 269 \tLoss: 1.4686102867126465\nEpoch: 4 \tBatch: 270 \tLoss: 1.4615520238876343\nEpoch: 4 \tBatch: 271 \tLoss: 1.4704476594924927\nEpoch: 4 \tBatch: 272 \tLoss: 1.48435640335083\nEpoch: 4 \tBatch: 273 \tLoss: 1.4693117141723633\nEpoch: 4 \tBatch: 274 \tLoss: 1.4675071239471436\nEpoch: 4 \tBatch: 275 \tLoss: 1.4685324430465698\nEpoch: 4 \tBatch: 276 \tLoss: 1.4694327116012573\nEpoch: 4 \tBatch: 277 \tLoss: 1.4791834354400635\nEpoch: 4 \tBatch: 278 \tLoss: 1.4770764112472534\nEpoch: 4 \tBatch: 279 \tLoss: 1.4625591039657593\nEpoch: 4 \tBatch: 280 \tLoss: 1.4690533876419067\nEpoch: 4 \tBatch: 281 \tLoss: 1.4716287851333618\nEpoch: 4 \tBatch: 282 \tLoss: 1.4827488660812378\nEpoch: 4 \tBatch: 283 \tLoss: 1.4611577987670898\nEpoch: 4 \tBatch: 284 \tLoss: 1.4842593669891357\nEpoch: 4 \tBatch: 285 \tLoss: 1.4689908027648926\nEpoch: 4 \tBatch: 286 \tLoss: 1.4689773321151733\nEpoch: 4 \tBatch: 287 \tLoss: 1.462822675704956\nEpoch: 4 \tBatch: 288 \tLoss: 1.473617672920227\nEpoch: 4 \tBatch: 289 \tLoss: 1.4912011623382568\nEpoch: 4 \tBatch: 290 \tLoss: 1.4620788097381592\nEpoch: 4 \tBatch: 291 \tLoss: 1.4644203186035156\nEpoch: 4 \tBatch: 292 \tLoss: 1.475767731666565\nEpoch: 4 \tBatch: 293 \tLoss: 1.4706635475158691\nEpoch: 4 \tBatch: 294 \tLoss: 1.4701836109161377\nEpoch: 4 \tBatch: 295 \tLoss: 1.4677983522415161\nEpoch: 4 \tBatch: 296 \tLoss: 1.4612315893173218\nEpoch: 4 \tBatch: 297 \tLoss: 1.4940866231918335\nEpoch: 4 \tBatch: 298 \tLoss: 1.4611667394638062\nEpoch: 4 \tBatch: 299 \tLoss: 1.4633628129959106\nEpoch: 4 \tBatch: 300 \tLoss: 1.4620553255081177\nEpoch: 4 \tBatch: 301 \tLoss: 1.4855360984802246\nEpoch: 4 \tBatch: 302 \tLoss: 1.4665149450302124\nEpoch: 4 \tBatch: 303 \tLoss: 1.4688091278076172\nEpoch: 4 \tBatch: 304 \tLoss: 1.4643123149871826\nEpoch: 4 \tBatch: 305 \tLoss: 1.4853566884994507\nEpoch: 4 \tBatch: 306 \tLoss: 1.4611834287643433\nEpoch: 4 \tBatch: 307 \tLoss: 1.4696929454803467\nEpoch: 4 \tBatch: 308 \tLoss: 1.4769132137298584\nEpoch: 4 \tBatch: 309 \tLoss: 1.4745222330093384\nEpoch: 4 \tBatch: 310 \tLoss: 1.4780049324035645\nEpoch: 4 \tBatch: 311 \tLoss: 1.461251139640808\nEpoch: 4 \tBatch: 312 \tLoss: 1.4873777627944946\nEpoch: 4 \tBatch: 313 \tLoss: 1.4689643383026123\nEpoch: 4 \tBatch: 314 \tLoss: 1.4788435697555542\nEpoch: 4 \tBatch: 315 \tLoss: 1.468051791191101\nEpoch: 4 \tBatch: 316 \tLoss: 1.4616084098815918\nEpoch: 4 \tBatch: 317 \tLoss: 1.4611619710922241\nEpoch: 4 \tBatch: 318 \tLoss: 1.4712765216827393\nEpoch: 4 \tBatch: 319 \tLoss: 1.4616166353225708\nEpoch: 4 \tBatch: 320 \tLoss: 1.461341142654419\nEpoch: 4 \tBatch: 321 \tLoss: 1.467482328414917\nEpoch: 4 \tBatch: 322 \tLoss: 1.4689723253250122\nEpoch: 4 \tBatch: 323 \tLoss: 1.4647250175476074\nEpoch: 4 \tBatch: 324 \tLoss: 1.469618797302246\nEpoch: 4 \tBatch: 325 \tLoss: 1.4766874313354492\nEpoch: 4 \tBatch: 326 \tLoss: 1.461493968963623\nEpoch: 4 \tBatch: 327 \tLoss: 1.4716829061508179\nEpoch: 4 \tBatch: 328 \tLoss: 1.4623162746429443\nEpoch: 4 \tBatch: 329 \tLoss: 1.496612787246704\nEpoch: 4 \tBatch: 330 \tLoss: 1.479475975036621\nEpoch: 4 \tBatch: 331 \tLoss: 1.4691660404205322\nEpoch: 4 \tBatch: 332 \tLoss: 1.4687443971633911\nEpoch: 4 \tBatch: 333 \tLoss: 1.477345585823059\nEpoch: 4 \tBatch: 334 \tLoss: 1.4693275690078735\nEpoch: 4 \tBatch: 335 \tLoss: 1.4804340600967407\nEpoch: 4 \tBatch: 336 \tLoss: 1.4689698219299316\nEpoch: 4 \tBatch: 337 \tLoss: 1.47149658203125\nEpoch: 4 \tBatch: 338 \tLoss: 1.472541093826294\nEpoch: 4 \tBatch: 339 \tLoss: 1.4784947633743286\nEpoch: 4 \tBatch: 340 \tLoss: 1.4777172803878784\nEpoch: 4 \tBatch: 341 \tLoss: 1.4692388772964478\nEpoch: 4 \tBatch: 342 \tLoss: 1.4697353839874268\nEpoch: 4 \tBatch: 343 \tLoss: 1.4778484106063843\nEpoch: 4 \tBatch: 344 \tLoss: 1.4697940349578857\nEpoch: 4 \tBatch: 345 \tLoss: 1.4749778509140015\nEpoch: 4 \tBatch: 346 \tLoss: 1.4632304906845093\nEpoch: 4 \tBatch: 347 \tLoss: 1.4692015647888184\nEpoch: 4 \tBatch: 348 \tLoss: 1.4690388441085815\nEpoch: 4 \tBatch: 349 \tLoss: 1.4710893630981445\nEpoch: 4 \tBatch: 350 \tLoss: 1.469031572341919\nEpoch: 4 \tBatch: 351 \tLoss: 1.4611644744873047\nEpoch: 4 \tBatch: 352 \tLoss: 1.4632385969161987\nEpoch: 4 \tBatch: 353 \tLoss: 1.4615700244903564\nEpoch: 4 \tBatch: 354 \tLoss: 1.4858503341674805\nEpoch: 4 \tBatch: 355 \tLoss: 1.4613227844238281\nEpoch: 4 \tBatch: 356 \tLoss: 1.4612104892730713\nEpoch: 4 \tBatch: 357 \tLoss: 1.468969702720642\nEpoch: 4 \tBatch: 358 \tLoss: 1.4613193273544312\nEpoch: 4 \tBatch: 359 \tLoss: 1.467898964881897\nEpoch: 4 \tBatch: 360 \tLoss: 1.469333291053772\nEpoch: 4 \tBatch: 361 \tLoss: 1.468969464302063\nEpoch: 4 \tBatch: 362 \tLoss: 1.4939981698989868\nEpoch: 4 \tBatch: 363 \tLoss: 1.468963384628296\nEpoch: 4 \tBatch: 364 \tLoss: 1.4619218111038208\nEpoch: 4 \tBatch: 365 \tLoss: 1.4769625663757324\nEpoch: 4 \tBatch: 366 \tLoss: 1.4780281782150269\nEpoch: 4 \tBatch: 367 \tLoss: 1.477011799812317\nEpoch: 4 \tBatch: 368 \tLoss: 1.4685003757476807\nEpoch: 4 \tBatch: 369 \tLoss: 1.4893484115600586\nEpoch: 4 \tBatch: 370 \tLoss: 1.4617626667022705\nEpoch: 4 \tBatch: 371 \tLoss: 1.4616910219192505\nEpoch: 4 \tBatch: 372 \tLoss: 1.4690967798233032\nEpoch: 4 \tBatch: 373 \tLoss: 1.4624354839324951\nEpoch: 4 \tBatch: 374 \tLoss: 1.4689794778823853\nEpoch: 4 \tBatch: 375 \tLoss: 1.4612016677856445\nEpoch: 4 \tBatch: 376 \tLoss: 1.4767509698867798\nEpoch: 4 \tBatch: 377 \tLoss: 1.4635740518569946\nEpoch: 4 \tBatch: 378 \tLoss: 1.4718090295791626\nEpoch: 4 \tBatch: 379 \tLoss: 1.4688917398452759\nEpoch: 4 \tBatch: 380 \tLoss: 1.4612746238708496\nEpoch: 4 \tBatch: 381 \tLoss: 1.4860221147537231\nEpoch: 4 \tBatch: 382 \tLoss: 1.4834476709365845\nEpoch: 4 \tBatch: 383 \tLoss: 1.474955677986145\nEpoch: 4 \tBatch: 384 \tLoss: 1.4612358808517456\nEpoch: 4 \tBatch: 385 \tLoss: 1.4689176082611084\nEpoch: 4 \tBatch: 386 \tLoss: 1.470223307609558\nEpoch: 4 \tBatch: 387 \tLoss: 1.4611603021621704\nEpoch: 4 \tBatch: 388 \tLoss: 1.4769072532653809\nEpoch: 4 \tBatch: 389 \tLoss: 1.464343547821045\nEpoch: 4 \tBatch: 390 \tLoss: 1.4611542224884033\nTraining Complete. Final loss = 1.4611542224884033\nTrain Accuracy: 99.316\nValidation Accuracy: 98.54\nTest Accuracy: 98.63\n"
    }
   ],
   "source": [
    "#Set argument batch_norm to True\n",
    "cnn2 = ConvNet(batch_norm=True).to(device)\n",
    "optimiser = torch.optim.Adam(cnn2.parameters(), lr=learning_rate)\n",
    "writer2 = SummaryWriter(log_dir=\"runs/cnn_bn_lr0005\")\n",
    "train(cnn2, epochs, writer=writer2)\n",
    "print('Train Accuracy:', calc_accuracy(cnn2, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn2, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn2, test_loader))"
   ]
  },
  {
   "source": [
    "From the plot in tensorboard, you should see that the convergence happens a lot earlier. The model therefore seems to be more robust from the beginning. Next, you can try using a higher learing rate. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": ": 1.4697396755218506\nEpoch: 3 \tBatch: 355 \tLoss: 1.4726462364196777\nEpoch: 3 \tBatch: 356 \tLoss: 1.4646005630493164\nEpoch: 3 \tBatch: 357 \tLoss: 1.4698542356491089\nEpoch: 3 \tBatch: 358 \tLoss: 1.4690487384796143\nEpoch: 3 \tBatch: 359 \tLoss: 1.4615857601165771\nEpoch: 3 \tBatch: 360 \tLoss: 1.4766308069229126\nEpoch: 3 \tBatch: 361 \tLoss: 1.4773776531219482\nEpoch: 3 \tBatch: 362 \tLoss: 1.4771806001663208\nEpoch: 3 \tBatch: 363 \tLoss: 1.4713208675384521\nEpoch: 3 \tBatch: 364 \tLoss: 1.46147620677948\nEpoch: 3 \tBatch: 365 \tLoss: 1.4868123531341553\nEpoch: 3 \tBatch: 366 \tLoss: 1.465869426727295\nEpoch: 3 \tBatch: 367 \tLoss: 1.486151099205017\nEpoch: 3 \tBatch: 368 \tLoss: 1.4854943752288818\nEpoch: 3 \tBatch: 369 \tLoss: 1.469657301902771\nEpoch: 3 \tBatch: 370 \tLoss: 1.4883553981781006\nEpoch: 3 \tBatch: 371 \tLoss: 1.476812481880188\nEpoch: 3 \tBatch: 372 \tLoss: 1.4651477336883545\nEpoch: 3 \tBatch: 373 \tLoss: 1.4831621646881104\nEpoch: 3 \tBatch: 374 \tLoss: 1.474079966545105\nEpoch: 3 \tBatch: 375 \tLoss: 1.4694504737854004\nEpoch: 3 \tBatch: 376 \tLoss: 1.4689915180206299\nEpoch: 3 \tBatch: 377 \tLoss: 1.4628417491912842\nEpoch: 3 \tBatch: 378 \tLoss: 1.4721732139587402\nEpoch: 3 \tBatch: 379 \tLoss: 1.461173415184021\nEpoch: 3 \tBatch: 380 \tLoss: 1.476733684539795\nEpoch: 3 \tBatch: 381 \tLoss: 1.4778070449829102\nEpoch: 3 \tBatch: 382 \tLoss: 1.4703187942504883\nEpoch: 3 \tBatch: 383 \tLoss: 1.465504765510559\nEpoch: 3 \tBatch: 384 \tLoss: 1.4617125988006592\nEpoch: 3 \tBatch: 385 \tLoss: 1.4852380752563477\nEpoch: 3 \tBatch: 386 \tLoss: 1.4768805503845215\nEpoch: 3 \tBatch: 387 \tLoss: 1.4693593978881836\nEpoch: 3 \tBatch: 388 \tLoss: 1.4808423519134521\nEpoch: 3 \tBatch: 389 \tLoss: 1.4614760875701904\nEpoch: 3 \tBatch: 390 \tLoss: 1.4766286611557007\nEpoch: 4 \tBatch: 0 \tLoss: 1.4946300983428955\nEpoch: 4 \tBatch: 1 \tLoss: 1.4756815433502197\nEpoch: 4 \tBatch: 2 \tLoss: 1.4645198583602905\nEpoch: 4 \tBatch: 3 \tLoss: 1.4674358367919922\nEpoch: 4 \tBatch: 4 \tLoss: 1.4629038572311401\nEpoch: 4 \tBatch: 5 \tLoss: 1.4690780639648438\nEpoch: 4 \tBatch: 6 \tLoss: 1.4766216278076172\nEpoch: 4 \tBatch: 7 \tLoss: 1.4778242111206055\nEpoch: 4 \tBatch: 8 \tLoss: 1.4890940189361572\nEpoch: 4 \tBatch: 9 \tLoss: 1.468136191368103\nEpoch: 4 \tBatch: 10 \tLoss: 1.4751944541931152\nEpoch: 4 \tBatch: 11 \tLoss: 1.469606876373291\nEpoch: 4 \tBatch: 12 \tLoss: 1.465159296989441\nEpoch: 4 \tBatch: 13 \tLoss: 1.468285083770752\nEpoch: 4 \tBatch: 14 \tLoss: 1.467079758644104\nEpoch: 4 \tBatch: 15 \tLoss: 1.4649344682693481\nEpoch: 4 \tBatch: 16 \tLoss: 1.4621703624725342\nEpoch: 4 \tBatch: 17 \tLoss: 1.4741432666778564\nEpoch: 4 \tBatch: 18 \tLoss: 1.4723931550979614\nEpoch: 4 \tBatch: 19 \tLoss: 1.4666568040847778\nEpoch: 4 \tBatch: 20 \tLoss: 1.461679220199585\nEpoch: 4 \tBatch: 21 \tLoss: 1.4690485000610352\nEpoch: 4 \tBatch: 22 \tLoss: 1.4649487733840942\nEpoch: 4 \tBatch: 23 \tLoss: 1.4620484113693237\nEpoch: 4 \tBatch: 24 \tLoss: 1.4642565250396729\nEpoch: 4 \tBatch: 25 \tLoss: 1.4715123176574707\nEpoch: 4 \tBatch: 26 \tLoss: 1.4619181156158447\nEpoch: 4 \tBatch: 27 \tLoss: 1.4732171297073364\nEpoch: 4 \tBatch: 28 \tLoss: 1.4611576795578003\nEpoch: 4 \tBatch: 29 \tLoss: 1.472307562828064\nEpoch: 4 \tBatch: 30 \tLoss: 1.4675097465515137\nEpoch: 4 \tBatch: 31 \tLoss: 1.465914011001587\nEpoch: 4 \tBatch: 32 \tLoss: 1.4703510999679565\nEpoch: 4 \tBatch: 33 \tLoss: 1.4689505100250244\nEpoch: 4 \tBatch: 34 \tLoss: 1.4676121473312378\nEpoch: 4 \tBatch: 35 \tLoss: 1.4615750312805176\nEpoch: 4 \tBatch: 36 \tLoss: 1.4736576080322266\nEpoch: 4 \tBatch: 37 \tLoss: 1.4841638803482056\nEpoch: 4 \tBatch: 38 \tLoss: 1.4769474267959595\nEpoch: 4 \tBatch: 39 \tLoss: 1.4695994853973389\nEpoch: 4 \tBatch: 40 \tLoss: 1.4614397287368774\nEpoch: 4 \tBatch: 41 \tLoss: 1.4723126888275146\nEpoch: 4 \tBatch: 42 \tLoss: 1.4618678092956543\nEpoch: 4 \tBatch: 43 \tLoss: 1.466623067855835\nEpoch: 4 \tBatch: 44 \tLoss: 1.4924551248550415\nEpoch: 4 \tBatch: 45 \tLoss: 1.4651193618774414\nEpoch: 4 \tBatch: 46 \tLoss: 1.4689021110534668\nEpoch: 4 \tBatch: 47 \tLoss: 1.5004469156265259\nEpoch: 4 \tBatch: 48 \tLoss: 1.4699867963790894\nEpoch: 4 \tBatch: 49 \tLoss: 1.4754434823989868\nEpoch: 4 \tBatch: 50 \tLoss: 1.4743071794509888\nEpoch: 4 \tBatch: 51 \tLoss: 1.4915664196014404\nEpoch: 4 \tBatch: 52 \tLoss: 1.4863369464874268\nEpoch: 4 \tBatch: 53 \tLoss: 1.491308569908142\nEpoch: 4 \tBatch: 54 \tLoss: 1.4830331802368164\nEpoch: 4 \tBatch: 55 \tLoss: 1.4729843139648438\nEpoch: 4 \tBatch: 56 \tLoss: 1.4617193937301636\nEpoch: 4 \tBatch: 57 \tLoss: 1.490458607673645\nEpoch: 4 \tBatch: 58 \tLoss: 1.4612653255462646\nEpoch: 4 \tBatch: 59 \tLoss: 1.4635471105575562\nEpoch: 4 \tBatch: 60 \tLoss: 1.5005642175674438\nEpoch: 4 \tBatch: 61 \tLoss: 1.478166103363037\nEpoch: 4 \tBatch: 62 \tLoss: 1.4645384550094604\nEpoch: 4 \tBatch: 63 \tLoss: 1.4683789014816284\nEpoch: 4 \tBatch: 64 \tLoss: 1.4814995527267456\nEpoch: 4 \tBatch: 65 \tLoss: 1.4691424369812012\nEpoch: 4 \tBatch: 66 \tLoss: 1.4805750846862793\nEpoch: 4 \tBatch: 67 \tLoss: 1.469759464263916\nEpoch: 4 \tBatch: 68 \tLoss: 1.4624491930007935\nEpoch: 4 \tBatch: 69 \tLoss: 1.4693939685821533\nEpoch: 4 \tBatch: 70 \tLoss: 1.4723167419433594\nEpoch: 4 \tBatch: 71 \tLoss: 1.4689658880233765\nEpoch: 4 \tBatch: 72 \tLoss: 1.4615838527679443\nEpoch: 4 \tBatch: 73 \tLoss: 1.463057279586792\nEpoch: 4 \tBatch: 74 \tLoss: 1.468964695930481\nEpoch: 4 \tBatch: 75 \tLoss: 1.462422490119934\nEpoch: 4 \tBatch: 76 \tLoss: 1.46245276927948\nEpoch: 4 \tBatch: 77 \tLoss: 1.4696124792099\nEpoch: 4 \tBatch: 78 \tLoss: 1.4757152795791626\nEpoch: 4 \tBatch: 79 \tLoss: 1.462104082107544\nEpoch: 4 \tBatch: 80 \tLoss: 1.4766284227371216\nEpoch: 4 \tBatch: 81 \tLoss: 1.4674217700958252\nEpoch: 4 \tBatch: 82 \tLoss: 1.4787399768829346\nEpoch: 4 \tBatch: 83 \tLoss: 1.4824423789978027\nEpoch: 4 \tBatch: 84 \tLoss: 1.4921530485153198\nEpoch: 4 \tBatch: 85 \tLoss: 1.466067910194397\nEpoch: 4 \tBatch: 86 \tLoss: 1.4774409532546997\nEpoch: 4 \tBatch: 87 \tLoss: 1.475122332572937\nEpoch: 4 \tBatch: 88 \tLoss: 1.4615236520767212\nEpoch: 4 \tBatch: 89 \tLoss: 1.4736579656600952\nEpoch: 4 \tBatch: 90 \tLoss: 1.4716259241104126\nEpoch: 4 \tBatch: 91 \tLoss: 1.4621551036834717\nEpoch: 4 \tBatch: 92 \tLoss: 1.478488802909851\nEpoch: 4 \tBatch: 93 \tLoss: 1.4612808227539062\nEpoch: 4 \tBatch: 94 \tLoss: 1.4703700542449951\nEpoch: 4 \tBatch: 95 \tLoss: 1.470157504081726\nEpoch: 4 \tBatch: 96 \tLoss: 1.462664008140564\nEpoch: 4 \tBatch: 97 \tLoss: 1.4611929655075073\nEpoch: 4 \tBatch: 98 \tLoss: 1.4704612493515015\nEpoch: 4 \tBatch: 99 \tLoss: 1.4937384128570557\nEpoch: 4 \tBatch: 100 \tLoss: 1.4694437980651855\nEpoch: 4 \tBatch: 101 \tLoss: 1.4685693979263306\nEpoch: 4 \tBatch: 102 \tLoss: 1.4734524488449097\nEpoch: 4 \tBatch: 103 \tLoss: 1.4631603956222534\nEpoch: 4 \tBatch: 104 \tLoss: 1.4658269882202148\nEpoch: 4 \tBatch: 105 \tLoss: 1.4699631929397583\nEpoch: 4 \tBatch: 106 \tLoss: 1.470495581626892\nEpoch: 4 \tBatch: 107 \tLoss: 1.4614611864089966\nEpoch: 4 \tBatch: 108 \tLoss: 1.4613476991653442\nEpoch: 4 \tBatch: 109 \tLoss: 1.4770721197128296\nEpoch: 4 \tBatch: 110 \tLoss: 1.477483868598938\nEpoch: 4 \tBatch: 111 \tLoss: 1.4765491485595703\nEpoch: 4 \tBatch: 112 \tLoss: 1.47175133228302\nEpoch: 4 \tBatch: 113 \tLoss: 1.469126582145691\nEpoch: 4 \tBatch: 114 \tLoss: 1.481077790260315\nEpoch: 4 \tBatch: 115 \tLoss: 1.4964804649353027\nEpoch: 4 \tBatch: 116 \tLoss: 1.4760899543762207\nEpoch: 4 \tBatch: 117 \tLoss: 1.4692126512527466\nEpoch: 4 \tBatch: 118 \tLoss: 1.4769296646118164\nEpoch: 4 \tBatch: 119 \tLoss: 1.4643590450286865\nEpoch: 4 \tBatch: 120 \tLoss: 1.491190791130066\nEpoch: 4 \tBatch: 121 \tLoss: 1.470069408416748\nEpoch: 4 \tBatch: 122 \tLoss: 1.4848525524139404\nEpoch: 4 \tBatch: 123 \tLoss: 1.4920859336853027\nEpoch: 4 \tBatch: 124 \tLoss: 1.4750339984893799\nEpoch: 4 \tBatch: 125 \tLoss: 1.477143406867981\nEpoch: 4 \tBatch: 126 \tLoss: 1.4702956676483154\nEpoch: 4 \tBatch: 127 \tLoss: 1.4852519035339355\nEpoch: 4 \tBatch: 128 \tLoss: 1.4730840921401978\nEpoch: 4 \tBatch: 129 \tLoss: 1.4715511798858643\nEpoch: 4 \tBatch: 130 \tLoss: 1.4795337915420532\nEpoch: 4 \tBatch: 131 \tLoss: 1.4892797470092773\nEpoch: 4 \tBatch: 132 \tLoss: 1.481529951095581\nEpoch: 4 \tBatch: 133 \tLoss: 1.4639486074447632\nEpoch: 4 \tBatch: 134 \tLoss: 1.4706840515136719\nEpoch: 4 \tBatch: 135 \tLoss: 1.4615579843521118\nEpoch: 4 \tBatch: 136 \tLoss: 1.4688224792480469\nEpoch: 4 \tBatch: 137 \tLoss: 1.47953462600708\nEpoch: 4 \tBatch: 138 \tLoss: 1.4620798826217651\nEpoch: 4 \tBatch: 139 \tLoss: 1.4710348844528198\nEpoch: 4 \tBatch: 140 \tLoss: 1.4627418518066406\nEpoch: 4 \tBatch: 141 \tLoss: 1.4720693826675415\nEpoch: 4 \tBatch: 142 \tLoss: 1.4690724611282349\nEpoch: 4 \tBatch: 143 \tLoss: 1.4689900875091553\nEpoch: 4 \tBatch: 144 \tLoss: 1.470324993133545\nEpoch: 4 \tBatch: 145 \tLoss: 1.4684607982635498\nEpoch: 4 \tBatch: 146 \tLoss: 1.4613022804260254\nEpoch: 4 \tBatch: 147 \tLoss: 1.4719547033309937\nEpoch: 4 \tBatch: 148 \tLoss: 1.4658334255218506\nEpoch: 4 \tBatch: 149 \tLoss: 1.4636774063110352\nEpoch: 4 \tBatch: 150 \tLoss: 1.4798966646194458\nEpoch: 4 \tBatch: 151 \tLoss: 1.4825323820114136\nEpoch: 4 \tBatch: 152 \tLoss: 1.4867528676986694\nEpoch: 4 \tBatch: 153 \tLoss: 1.4698596000671387\nEpoch: 4 \tBatch: 154 \tLoss: 1.4696614742279053\nEpoch: 4 \tBatch: 155 \tLoss: 1.4689159393310547\nEpoch: 4 \tBatch: 156 \tLoss: 1.4771995544433594\nEpoch: 4 \tBatch: 157 \tLoss: 1.4851371049880981\nEpoch: 4 \tBatch: 158 \tLoss: 1.4621152877807617\nEpoch: 4 \tBatch: 159 \tLoss: 1.4613591432571411\nEpoch: 4 \tBatch: 160 \tLoss: 1.462798833847046\nEpoch: 4 \tBatch: 161 \tLoss: 1.4638550281524658\nEpoch: 4 \tBatch: 162 \tLoss: 1.4920271635055542\nEpoch: 4 \tBatch: 163 \tLoss: 1.469355821609497\nEpoch: 4 \tBatch: 164 \tLoss: 1.4767658710479736\nEpoch: 4 \tBatch: 165 \tLoss: 1.4977415800094604\nEpoch: 4 \tBatch: 166 \tLoss: 1.4624764919281006\nEpoch: 4 \tBatch: 167 \tLoss: 1.4842184782028198\nEpoch: 4 \tBatch: 168 \tLoss: 1.4615224599838257\nEpoch: 4 \tBatch: 169 \tLoss: 1.469079613685608\nEpoch: 4 \tBatch: 170 \tLoss: 1.4728175401687622\nEpoch: 4 \tBatch: 171 \tLoss: 1.461916208267212\nEpoch: 4 \tBatch: 172 \tLoss: 1.47079336643219\nEpoch: 4 \tBatch: 173 \tLoss: 1.4697717428207397\nEpoch: 4 \tBatch: 174 \tLoss: 1.4621227979660034\nEpoch: 4 \tBatch: 175 \tLoss: 1.4768599271774292\nEpoch: 4 \tBatch: 176 \tLoss: 1.4675734043121338\nEpoch: 4 \tBatch: 177 \tLoss: 1.4699300527572632\nEpoch: 4 \tBatch: 178 \tLoss: 1.4613020420074463\nEpoch: 4 \tBatch: 179 \tLoss: 1.4784637689590454\nEpoch: 4 \tBatch: 180 \tLoss: 1.474158763885498\nEpoch: 4 \tBatch: 181 \tLoss: 1.4843240976333618\nEpoch: 4 \tBatch: 182 \tLoss: 1.4692245721817017\nEpoch: 4 \tBatch: 183 \tLoss: 1.491302728652954\nEpoch: 4 \tBatch: 184 \tLoss: 1.4767985343933105\nEpoch: 4 \tBatch: 185 \tLoss: 1.4758381843566895\nEpoch: 4 \tBatch: 186 \tLoss: 1.471017837524414\nEpoch: 4 \tBatch: 187 \tLoss: 1.4694952964782715\nEpoch: 4 \tBatch: 188 \tLoss: 1.4611735343933105\nEpoch: 4 \tBatch: 189 \tLoss: 1.4713221788406372\nEpoch: 4 \tBatch: 190 \tLoss: 1.4622604846954346\nEpoch: 4 \tBatch: 191 \tLoss: 1.4691247940063477\nEpoch: 4 \tBatch: 192 \tLoss: 1.4717767238616943\nEpoch: 4 \tBatch: 193 \tLoss: 1.4714409112930298\nEpoch: 4 \tBatch: 194 \tLoss: 1.4806828498840332\nEpoch: 4 \tBatch: 195 \tLoss: 1.4676918983459473\nEpoch: 4 \tBatch: 196 \tLoss: 1.4767311811447144\nEpoch: 4 \tBatch: 197 \tLoss: 1.4789942502975464\nEpoch: 4 \tBatch: 198 \tLoss: 1.47075617313385\nEpoch: 4 \tBatch: 199 \tLoss: 1.497684121131897\nEpoch: 4 \tBatch: 200 \tLoss: 1.4704164266586304\nEpoch: 4 \tBatch: 201 \tLoss: 1.4767225980758667\nEpoch: 4 \tBatch: 202 \tLoss: 1.464243769645691\nEpoch: 4 \tBatch: 203 \tLoss: 1.471469521522522\nEpoch: 4 \tBatch: 204 \tLoss: 1.4719852209091187\nEpoch: 4 \tBatch: 205 \tLoss: 1.4624147415161133\nEpoch: 4 \tBatch: 206 \tLoss: 1.4747464656829834\nEpoch: 4 \tBatch: 207 \tLoss: 1.4776474237442017\nEpoch: 4 \tBatch: 208 \tLoss: 1.4851768016815186\nEpoch: 4 \tBatch: 209 \tLoss: 1.4622148275375366\nEpoch: 4 \tBatch: 210 \tLoss: 1.468940019607544\nEpoch: 4 \tBatch: 211 \tLoss: 1.461174726486206\nEpoch: 4 \tBatch: 212 \tLoss: 1.461320161819458\nEpoch: 4 \tBatch: 213 \tLoss: 1.4621045589447021\nEpoch: 4 \tBatch: 214 \tLoss: 1.4693686962127686\nEpoch: 4 \tBatch: 215 \tLoss: 1.4686615467071533\nEpoch: 4 \tBatch: 216 \tLoss: 1.4844213724136353\nEpoch: 4 \tBatch: 217 \tLoss: 1.4796925783157349\nEpoch: 4 \tBatch: 218 \tLoss: 1.474039077758789\nEpoch: 4 \tBatch: 219 \tLoss: 1.4611914157867432\nEpoch: 4 \tBatch: 220 \tLoss: 1.4696812629699707\nEpoch: 4 \tBatch: 221 \tLoss: 1.4929639101028442\nEpoch: 4 \tBatch: 222 \tLoss: 1.476388692855835\nEpoch: 4 \tBatch: 223 \tLoss: 1.4723684787750244\nEpoch: 4 \tBatch: 224 \tLoss: 1.4681692123413086\nEpoch: 4 \tBatch: 225 \tLoss: 1.496944546699524\nEpoch: 4 \tBatch: 226 \tLoss: 1.4848092794418335\nEpoch: 4 \tBatch: 227 \tLoss: 1.4612597227096558\nEpoch: 4 \tBatch: 228 \tLoss: 1.4775480031967163\nEpoch: 4 \tBatch: 229 \tLoss: 1.474155068397522\nEpoch: 4 \tBatch: 230 \tLoss: 1.4670315980911255\nEpoch: 4 \tBatch: 231 \tLoss: 1.4746856689453125\nEpoch: 4 \tBatch: 232 \tLoss: 1.4841313362121582\nEpoch: 4 \tBatch: 233 \tLoss: 1.4694631099700928\nEpoch: 4 \tBatch: 234 \tLoss: 1.4637442827224731\nEpoch: 4 \tBatch: 235 \tLoss: 1.4615892171859741\nEpoch: 4 \tBatch: 236 \tLoss: 1.4773542881011963\nEpoch: 4 \tBatch: 237 \tLoss: 1.461937665939331\nEpoch: 4 \tBatch: 238 \tLoss: 1.4613093137741089\nEpoch: 4 \tBatch: 239 \tLoss: 1.4690783023834229\nEpoch: 4 \tBatch: 240 \tLoss: 1.4733270406723022\nEpoch: 4 \tBatch: 241 \tLoss: 1.4710347652435303\nEpoch: 4 \tBatch: 242 \tLoss: 1.491091251373291\nEpoch: 4 \tBatch: 243 \tLoss: 1.4614441394805908\nEpoch: 4 \tBatch: 244 \tLoss: 1.4768562316894531\nEpoch: 4 \tBatch: 245 \tLoss: 1.470885992050171\nEpoch: 4 \tBatch: 246 \tLoss: 1.4783707857131958\nEpoch: 4 \tBatch: 247 \tLoss: 1.4995017051696777\nEpoch: 4 \tBatch: 248 \tLoss: 1.467012643814087\nEpoch: 4 \tBatch: 249 \tLoss: 1.4703036546707153\nEpoch: 4 \tBatch: 250 \tLoss: 1.4701862335205078\nEpoch: 4 \tBatch: 251 \tLoss: 1.481945276260376\nEpoch: 4 \tBatch: 252 \tLoss: 1.4864252805709839\nEpoch: 4 \tBatch: 253 \tLoss: 1.4614551067352295\nEpoch: 4 \tBatch: 254 \tLoss: 1.4612644910812378\nEpoch: 4 \tBatch: 255 \tLoss: 1.4652644395828247\nEpoch: 4 \tBatch: 256 \tLoss: 1.476948857307434\nEpoch: 4 \tBatch: 257 \tLoss: 1.4780844449996948\nEpoch: 4 \tBatch: 258 \tLoss: 1.4611949920654297\nEpoch: 4 \tBatch: 259 \tLoss: 1.4690368175506592\nEpoch: 4 \tBatch: 260 \tLoss: 1.46897554397583\nEpoch: 4 \tBatch: 261 \tLoss: 1.4700589179992676\nEpoch: 4 \tBatch: 262 \tLoss: 1.4692139625549316\nEpoch: 4 \tBatch: 263 \tLoss: 1.4730324745178223\nEpoch: 4 \tBatch: 264 \tLoss: 1.4632325172424316\nEpoch: 4 \tBatch: 265 \tLoss: 1.4691565036773682\nEpoch: 4 \tBatch: 266 \tLoss: 1.479114294052124\nEpoch: 4 \tBatch: 267 \tLoss: 1.4649431705474854\nEpoch: 4 \tBatch: 268 \tLoss: 1.4878443479537964\nEpoch: 4 \tBatch: 269 \tLoss: 1.4707210063934326\nEpoch: 4 \tBatch: 270 \tLoss: 1.4666680097579956\nEpoch: 4 \tBatch: 271 \tLoss: 1.4693313837051392\nEpoch: 4 \tBatch: 272 \tLoss: 1.4748170375823975\nEpoch: 4 \tBatch: 273 \tLoss: 1.4822359085083008\nEpoch: 4 \tBatch: 274 \tLoss: 1.4789178371429443\nEpoch: 4 \tBatch: 275 \tLoss: 1.4843634366989136\nEpoch: 4 \tBatch: 276 \tLoss: 1.4774588346481323\nEpoch: 4 \tBatch: 277 \tLoss: 1.4694089889526367\nEpoch: 4 \tBatch: 278 \tLoss: 1.4692049026489258\nEpoch: 4 \tBatch: 279 \tLoss: 1.4916034936904907\nEpoch: 4 \tBatch: 280 \tLoss: 1.4778209924697876\nEpoch: 4 \tBatch: 281 \tLoss: 1.4725563526153564\nEpoch: 4 \tBatch: 282 \tLoss: 1.4625009298324585\nEpoch: 4 \tBatch: 283 \tLoss: 1.4686975479125977\nEpoch: 4 \tBatch: 284 \tLoss: 1.4690135717391968\nEpoch: 4 \tBatch: 285 \tLoss: 1.462493896484375\nEpoch: 4 \tBatch: 286 \tLoss: 1.4692192077636719\nEpoch: 4 \tBatch: 287 \tLoss: 1.4654830694198608\nEpoch: 4 \tBatch: 288 \tLoss: 1.4623026847839355\nEpoch: 4 \tBatch: 289 \tLoss: 1.475851058959961\nEpoch: 4 \tBatch: 290 \tLoss: 1.4745228290557861\nEpoch: 4 \tBatch: 291 \tLoss: 1.4744304418563843\nEpoch: 4 \tBatch: 292 \tLoss: 1.4811125993728638\nEpoch: 4 \tBatch: 293 \tLoss: 1.4845787286758423\nEpoch: 4 \tBatch: 294 \tLoss: 1.4824994802474976\nEpoch: 4 \tBatch: 295 \tLoss: 1.4828091859817505\nEpoch: 4 \tBatch: 296 \tLoss: 1.4780546426773071\nEpoch: 4 \tBatch: 297 \tLoss: 1.4830498695373535\nEpoch: 4 \tBatch: 298 \tLoss: 1.470796823501587\nEpoch: 4 \tBatch: 299 \tLoss: 1.4689363241195679\nEpoch: 4 \tBatch: 300 \tLoss: 1.4633227586746216\nEpoch: 4 \tBatch: 301 \tLoss: 1.4613579511642456\nEpoch: 4 \tBatch: 302 \tLoss: 1.4615713357925415\nEpoch: 4 \tBatch: 303 \tLoss: 1.4734247922897339\nEpoch: 4 \tBatch: 304 \tLoss: 1.4611884355545044\nEpoch: 4 \tBatch: 305 \tLoss: 1.4938480854034424\nEpoch: 4 \tBatch: 306 \tLoss: 1.469030499458313\nEpoch: 4 \tBatch: 307 \tLoss: 1.4676713943481445\nEpoch: 4 \tBatch: 308 \tLoss: 1.4774563312530518\nEpoch: 4 \tBatch: 309 \tLoss: 1.4868699312210083\nEpoch: 4 \tBatch: 310 \tLoss: 1.4692192077636719\nEpoch: 4 \tBatch: 311 \tLoss: 1.4611564874649048\nEpoch: 4 \tBatch: 312 \tLoss: 1.4765565395355225\nEpoch: 4 \tBatch: 313 \tLoss: 1.4768071174621582\nEpoch: 4 \tBatch: 314 \tLoss: 1.4619450569152832\nEpoch: 4 \tBatch: 315 \tLoss: 1.477746844291687\nEpoch: 4 \tBatch: 316 \tLoss: 1.4756678342819214\nEpoch: 4 \tBatch: 317 \tLoss: 1.4612752199172974\nEpoch: 4 \tBatch: 318 \tLoss: 1.471091866493225\nEpoch: 4 \tBatch: 319 \tLoss: 1.4860750436782837\nEpoch: 4 \tBatch: 320 \tLoss: 1.4689651727676392\nEpoch: 4 \tBatch: 321 \tLoss: 1.4745646715164185\nEpoch: 4 \tBatch: 322 \tLoss: 1.4784467220306396\nEpoch: 4 \tBatch: 323 \tLoss: 1.4655566215515137\nEpoch: 4 \tBatch: 324 \tLoss: 1.4611902236938477\nEpoch: 4 \tBatch: 325 \tLoss: 1.4691250324249268\nEpoch: 4 \tBatch: 326 \tLoss: 1.4612150192260742\nEpoch: 4 \tBatch: 327 \tLoss: 1.471775770187378\nEpoch: 4 \tBatch: 328 \tLoss: 1.4803353548049927\nEpoch: 4 \tBatch: 329 \tLoss: 1.47672700881958\nEpoch: 4 \tBatch: 330 \tLoss: 1.4857149124145508\nEpoch: 4 \tBatch: 331 \tLoss: 1.4620527029037476\nEpoch: 4 \tBatch: 332 \tLoss: 1.5051257610321045\nEpoch: 4 \tBatch: 333 \tLoss: 1.4667311906814575\nEpoch: 4 \tBatch: 334 \tLoss: 1.4636815786361694\nEpoch: 4 \tBatch: 335 \tLoss: 1.463057279586792\nEpoch: 4 \tBatch: 336 \tLoss: 1.4671447277069092\nEpoch: 4 \tBatch: 337 \tLoss: 1.478186011314392\nEpoch: 4 \tBatch: 338 \tLoss: 1.4751023054122925\nEpoch: 4 \tBatch: 339 \tLoss: 1.4640201330184937\nEpoch: 4 \tBatch: 340 \tLoss: 1.4694154262542725\nEpoch: 4 \tBatch: 341 \tLoss: 1.4611955881118774\nEpoch: 4 \tBatch: 342 \tLoss: 1.4611583948135376\nEpoch: 4 \tBatch: 343 \tLoss: 1.4709354639053345\nEpoch: 4 \tBatch: 344 \tLoss: 1.4696625471115112\nEpoch: 4 \tBatch: 345 \tLoss: 1.4846922159194946\nEpoch: 4 \tBatch: 346 \tLoss: 1.4664051532745361\nEpoch: 4 \tBatch: 347 \tLoss: 1.476877212524414\nEpoch: 4 \tBatch: 348 \tLoss: 1.4689741134643555\nEpoch: 4 \tBatch: 349 \tLoss: 1.461700677871704\nEpoch: 4 \tBatch: 350 \tLoss: 1.4845479726791382\nEpoch: 4 \tBatch: 351 \tLoss: 1.4768383502960205\nEpoch: 4 \tBatch: 352 \tLoss: 1.4722596406936646\nEpoch: 4 \tBatch: 353 \tLoss: 1.4697840213775635\nEpoch: 4 \tBatch: 354 \tLoss: 1.47409987449646\nEpoch: 4 \tBatch: 355 \tLoss: 1.4769670963287354\nEpoch: 4 \tBatch: 356 \tLoss: 1.4700697660446167\nEpoch: 4 \tBatch: 357 \tLoss: 1.464060664176941\nEpoch: 4 \tBatch: 358 \tLoss: 1.4688974618911743\nEpoch: 4 \tBatch: 359 \tLoss: 1.4611961841583252\nEpoch: 4 \tBatch: 360 \tLoss: 1.4713821411132812\nEpoch: 4 \tBatch: 361 \tLoss: 1.4617693424224854\nEpoch: 4 \tBatch: 362 \tLoss: 1.464396595954895\nEpoch: 4 \tBatch: 363 \tLoss: 1.4768221378326416\nEpoch: 4 \tBatch: 364 \tLoss: 1.4612209796905518\nEpoch: 4 \tBatch: 365 \tLoss: 1.4698599576950073\nEpoch: 4 \tBatch: 366 \tLoss: 1.462604284286499\nEpoch: 4 \tBatch: 367 \tLoss: 1.4846714735031128\nEpoch: 4 \tBatch: 368 \tLoss: 1.4677249193191528\nEpoch: 4 \tBatch: 369 \tLoss: 1.4801079034805298\nEpoch: 4 \tBatch: 370 \tLoss: 1.467362642288208\nEpoch: 4 \tBatch: 371 \tLoss: 1.4613609313964844\nEpoch: 4 \tBatch: 372 \tLoss: 1.4754588603973389\nEpoch: 4 \tBatch: 373 \tLoss: 1.4680588245391846\nEpoch: 4 \tBatch: 374 \tLoss: 1.4634156227111816\nEpoch: 4 \tBatch: 375 \tLoss: 1.4686609506607056\nEpoch: 4 \tBatch: 376 \tLoss: 1.4618980884552002\nEpoch: 4 \tBatch: 377 \tLoss: 1.4624391794204712\nEpoch: 4 \tBatch: 378 \tLoss: 1.4846221208572388\nEpoch: 4 \tBatch: 379 \tLoss: 1.4628283977508545\nEpoch: 4 \tBatch: 380 \tLoss: 1.4708530902862549\nEpoch: 4 \tBatch: 381 \tLoss: 1.4826189279556274\nEpoch: 4 \tBatch: 382 \tLoss: 1.4671146869659424\nEpoch: 4 \tBatch: 383 \tLoss: 1.4685112237930298\nEpoch: 4 \tBatch: 384 \tLoss: 1.4716145992279053\nEpoch: 4 \tBatch: 385 \tLoss: 1.461151123046875\nEpoch: 4 \tBatch: 386 \tLoss: 1.4787960052490234\nEpoch: 4 \tBatch: 387 \tLoss: 1.4612032175064087\nEpoch: 4 \tBatch: 388 \tLoss: 1.4615247249603271\nEpoch: 4 \tBatch: 389 \tLoss: 1.4743282794952393\nEpoch: 4 \tBatch: 390 \tLoss: 1.469662070274353\nTraining Complete. Final loss = 1.469662070274353\nTrain Accuracy: 98.956\nValidation Accuracy: 99.03\nTest Accuracy: 98.42\n"
    }
   ],
   "source": [
    "#Try a larger learning rate\n",
    "cnn3 = ConvNet(batch_norm=True).to(device)\n",
    "optimiser = torch.optim.Adam(cnn3.parameters(), lr=0.05) #before lr=0.0005\n",
    "writer3 = SummaryWriter(log_dir=\"runs/cnn_bn_lr05\")\n",
    "train(cnn2, epochs, writer=writer3)\n",
    "print('Train Accuracy:', calc_accuracy(cnn2, train_loader))\n",
    "print('Validation Accuracy:', calc_accuracy(cnn2, val_loader))\n",
    "print('Test Accuracy:', calc_accuracy(cnn2, test_loader))"
   ]
  },
  {
   "source": [
    "![Loss Comparison](images/loss.png)\n",
    "From the three runs, we can see that the vanilla CNN (orange) took the longest to converge. The CNN with batch normalisation (red) was much faster to do so and we were even able to use a larger learning rate (blue), which leads to even faster convergence, because we have a more robust cnn due to batch normalisation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "With batch normalisation, we can use larger learning rates and have to be less careful with the selection of initial parameters, which leads to faster convergence and better performance. The networks do not even need to be trained so long, hence we could introduce early stopping now. If you want to learn about early stopping, you can do so [here](https://www.kaggle.com/akhileshrai/tutorial-early-stopping-vanilla-rnn-pytorch). Batch normalisation even works as a regularisation technique. By estimating the mean and variance for each batch, we add noise to the input data of each following layer, which helps the regularisation capability of the network. <br>\n",
    "It should be used with caution in combination with small batch sizes as the estimation of the mean and variance per batch is unstable, if we only have a small batch sample to calculate these values from.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}