{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability\n",
    "\n",
    "### Learning Objectives:\n",
    "- [Random Variables](#Random-Variables)\n",
    "- [Introduction to Probability](#Introduction-to-Probability)\n",
    "- [Joint & Marginal Probability](#Joint-&-Marginal-Probability)\n",
    "- [Conditional Probability](#Conditional-Probability)\n",
    "- [Probability Chain Rule](#Probability-Chain-Rule)\n",
    "\n",
    "# Random Variables\n",
    "A __random variable__ is defined as a variable whose values depend on outcomes of random events. For example, when we carry out an experiment, we never get _exactly_ the same result twice, maybe due to some measurement errors or some flaws in the system we are using to measure. But what is __randomness__? \n",
    "\n",
    "Some argue that this variation is not random and simply due to high-complexity processes that we are in no place of following accurately. For example, Robert Brown, a famous physicist, saw that pollen particles in water seem to bounce around randomly by bouncing about the water molecules. Some would argue that this was not random, and that if we knew where __all__ water molecules were and where they were going at all times, we could predict exactly where they would be later on. However, this is impossible to do in practice, and there was far more success when treating these as random forces!\n",
    "\n",
    "How do we generate a random value? We can use in-built functions, such as the ones from the __random__ and __numpy__ modules to generate random values. Numpy comes with the added functionality of creating vectors and matrices of random values in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random module: 1 0.30932133271787954\n",
      "numpy module: 2.648683891569478 1.25695209880263\n",
      "\n",
      "[-0.02277836  1.64348629 -1.07424498 -0.25678944 -1.62266452]\n",
      "[[3.52113911 2.46897086]\n",
      " [3.67611489 3.87986537]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Generating randomness with random!\n",
    "random_number = random.randint(0,4) # random integer between 0 and 4\n",
    "random_number2 = random.random() # random floating point value between 0 and 1\n",
    "print(\"random module:\", random_number, random_number2)\n",
    "\n",
    "# Generating randomness with numpy!\n",
    "random_number = np.random.uniform(0,4) # random value from uniform distribution between 0 and 4\n",
    "random_number2 = np.random.normal(0,1) # random value from standard normal distribution\n",
    "print(\"numpy module:\",random_number, random_number2)\n",
    "\n",
    "print()\n",
    "\n",
    "# Generating a random vector and matrix\n",
    "random_vector = np.random.normal(0,1,5) #Â 1x5 vector from standard normal distribution\n",
    "random_matrix = np.random.uniform(0,4,(2,2)) # 2x2 matrixrandom values from uniform distribution between 0 and 4\n",
    "print(random_vector)\n",
    "print(random_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to random variables, we cannot predict exactly what values they will take, but rather, we focus on determining on the likelihood of taking different values using various statistical tools. But how do we quantify ___how likely___ a value is to take place?\n",
    "\n",
    "\n",
    "#  Introduction to Probability\n",
    "__Probability__ is a measure of how likely an outcome is to occur given all other possible outcomes and the given circumstances. Therefore, when dealing with random variables, we do not concern ourselves with __what will happen__, but instead with __the probability of given outcome(s), also known as events, occuring__. Probability is formally defined as:\n",
    "\n",
    "$$ \\text{Probability of an outcome} = \\frac{\\text{Number of wanted outcomes}}{\\text{Number of possible outcomes}} $$\n",
    "\n",
    "Intuitively, we know that the probability of something occurring has to be somewhere between 0, where the outcome _cannot_ occur, and 1, where the outcome _will_ occur, where everything else is somewhere in between. We can apply probability in the context of the likelihood of a random variable taking a value or being within a range of values.\n",
    "\n",
    "\n",
    "In general, we use the operator $P(A)$ to denote the probability that an event A takes place. Let us look at an example with random variables. Given a random variable X, and two values $x_{1}$ and $x_{2}$, we can also define $P(X=x_{1}\\cup X=x_{2})$ as the probability of ___either___ $X=x_{1}$ or $X=x_{2}$ taking place and $P(X=x_{1}\\cap X=x_{2})$ as the probability of ___both___ $X=x_{1}$ and $X=x_{2}$. In terms of logical operators, __union__( $\\cup$ ) corresponds to the __or__ operator and __intersection__( $\\cap$ ) to the __and__ operator. Using these, we can define key properties of probability theory: \n",
    "\n",
    "$$1. P(X=x_{1}) = 1-P(X=x_{1}') $$\n",
    "$$2. \\sum_{i=1}^{N}P(X=x_{i}) = 1$$\n",
    "$$3. P(X=x_{1}\\cup X=x_{2}) = P(X=x_{1}) + P(X=x_{2}) - P(X=x_{1}\\cap X=x_{2})$$\n",
    "\n",
    "### Maybe add a small Venn diagram so they picture it better.\n",
    "\n",
    "$X=x_{1} '$ is the __complement__ of $X=x_{1}$, and represents all other values X can take besides $x_{1}$. The complement behaves like the __not__ operator, meaning the first property implies that the probability of a value is equal to 1 minus the probability of __NOT__ getting that value. If the probability of raining tomorrow is 0.8, then the probability that it will NOT rain tomorrow is 1 - 0.8 = 0.2. The second property implies that the sum of the probabilities of all values a random variable can take must equal to 1. This makes sense, since out of all possible outcomes, at least one must take place. For example, there is a probability of 1 that it will either rain or not tomorrow, and there is a probability of 1 that the height of a human we pick at random will be between 0 and infinity! The third property shows that the probability of X being equal to either $x_{1}$ _or_ $x_{2}$ occuring is given by the _sum_ of the probability of $X=x_{1}$ with the probability of $X=x_{2}$, subtracted by the probability of X taking both values. This is because the by adding the two individual probabilities, the intersection between them is accounted for twice. If the two conditions cannot be true simultaneously, they are known as __mutually exclusive__. For instance, it cannot rain _and_ not rain at the same time, so these are mutually exclusive events. \n",
    "\n",
    "\n",
    "So when we look at the probability in the context of random variables, we can determine the probability of a random variable taking different values. However, we can also consider the probability of two or more random variables simultaneously.\n",
    "\n",
    "\n",
    "# Joint & Marginal Probability\n",
    "\n",
    "Rather than only considering the probabilities associated with one random variable, we can also look at the probabilities associated with a pair of random variables at specific values, or any set of random variables. The probability of multiple random variables taking specific values is known as a __joint probability__. This is an interesting property in understanding the relationship between these variables. If we consider two random variables, X and Y, their joint probability is, in general, denoted as follows:\n",
    "\n",
    "$$p(x,y) = P(X=x \\;and\\; Y=y)$$\n",
    "\n",
    "\n",
    "Consider the example data shown below, based on the random variables \"AI Core Graduate\" and \"Jobs In Data Science\":\n",
    "\n",
    "|                               | AI Core Graduate | Non-AI Core Graduate | Total |\n",
    "|-------------------------------|------------------|----------------------|-------|\n",
    "| Jobs In Data Science    | 120              | 30                   | 150   |\n",
    "| No Jobs In Data Science | 0                | 150                  | 150   |\n",
    "| Total                         | 120              | 180                  | 300   |\n",
    "\n",
    "We have seen above that to calculate a probability, we simply divide the number of wanted outcomes by the total number of outcomes. Therefore, to calculate the probability of each individual combination of values of our random variables, we divide by the total number of outcomes, which in this case is 300. This leads to the table below:\n",
    "\n",
    "|                               | AI Core Graduate | Non-AI Core Graduate | Total |\n",
    "|-------------------------------|------------------|----------------------|-------|\n",
    "| Jobs In Data Science    | 0.4              | 0.1                  | 0.5   |\n",
    "| Jobs In Data Science | 0.0              | 0.5                  | 0.5   |\n",
    "| Total                         | 0.4              | 0.6                  | 1     |\n",
    "\n",
    "This is now a table of __joint probabilities__. Let us rewrite these results in their general form:\n",
    "\n",
    "$$p(\\text{AI Core Graduate, Jobs In Data Science}) = 0.4$$\n",
    "\n",
    "$$p(\\text{AI Core Graduate, No Jobs In Data Science}) = 0.0$$\n",
    "\n",
    "$$p(\\text{Non-AI Core Graduate, Jobs In Data Science}) = 0.1$$\n",
    "\n",
    "$$p(\\text{Non-AI Core Graduate, Jobs In Data Science}) = 0.5$$\n",
    "\n",
    "This example helps us understand the importance of joint probailities, as in this case, the different joint probabilities tell us that it is way more likely to have a job in data science if you are an AI Core graduate than if you are not a graduate!\n",
    "\n",
    "However, sometimes we are only concerned with probability of one of the two (or more) random variables taking a particular variable. For example, what if we wanted to know from our data what the probability that someone being an AI Core graduate is? what is the probability of someone having a job in data science, regardless of whether they've studied with us or not? These are known as __marginal probabilities.__ In general, for two random variables, X and Y, we can calculate their marginal probabilities, $p_{X}(x)$ and $p_{Y}(y)$, as follows:\n",
    "\n",
    "$$p_{X}(x) = \\sum_{y}p(x,y)$$\n",
    "\n",
    "$$p_{Y}{y} = \\sum_{x}p(x,y)$$\n",
    "\n",
    "This means that for that marginal probability of a random variable X, we simply __sum joint probabilities of all possible values that other random variables can take__. The reverse applies for the case of the random variable Y. This means that in our case, we can calculate the marginal probabilities as follows:\n",
    "\n",
    "$$p(\\text{AI Core Graduate}) = p(\\text{AI Core Graduate, Jobs In Data Science}) + p(\\text{AI Core Graduate, No Jobs In Data Science}) = 0+ 0.4 = 0.4$$\n",
    "\n",
    "$$p(\\text{Great Jobs In Data Science}) = p(\\text{AI Core Graduate, Jobs In Data Science}) + p(\\text{Non-AI Core Graduate, Jobs In Data Science}) = 0.4 + 0.1 = 0.5$$\n",
    "\n",
    "\n",
    "# Conditional Probability\n",
    "While random variables, are, as the name implies, random, probabilities are the measure of the likelihood of an event occuring. Under different circumstances, things may become more or less likely to happen. Given that it is sunny now, the probability of raining soon is smaller than if it was cloudy. This leads to the field of __conditional probability__, which is the probability of an event taking place ___given___ another event has occured. In the context of the joint probability of two random varaibles, X and Y, we consider $P(X=x_{1}|Y=y_{1})$ to represent the probability of $X=x_{1}$ occuring _given that_ $Y=y_{1}$, we can define it with the equation below.\n",
    "\n",
    "$$P(X=x_{1}|Y=y_{1}) = p(x_{1}|y_{1}) = \\frac{P(X=x_{1}\\cap Y=y_{1})}{P(Y=y_{1})} = \\frac{p(x,y)}{p_{Y}(y)}$$\n",
    "\n",
    "While there is no formal proof for the equation above, we can make intuitive sense of it. There is a probability associated with the uncertainty of whether $Y=y_{1}$ or not. So if we assume that X is __dependent__ on Y, meaning the probability of X taking a given value depends on what value Y will take, knowing the value of Y removes some uncertainty from what value X will take. \n",
    "\n",
    "For example, let's say that I'm a birdwatcher following a rare yellow flamingo. My goal is to find the flamingo, and take a picture of him that we can publish on the _BirdsBirdsBirds Weekly_ magazine. For my success, both the event of finding it and getting a good picture need to take place. However, if I have already found the yellow flamingo, the probability of getting a good picture that day increases proportionally to how hard it was to find it in the first place!\n",
    "\n",
    "We can also establish the __law of total probability__ by elaborating on the equation for marginal probability, which is given by:\n",
    "\n",
    "$$p_{X}(x) = \\sum_{i=1}^{N}p(x|y_{i})p_{Y}(y) $$\n",
    "\n",
    "Where N is the total number of prior outcomes, $y_{i}$ is the $i^{th}$ possible value of Y. If we analyse the equation more closely, we can see that the sum of the probabilities of $y_{i}$ and $x$ both occuring are the individual components that make up the probability of B taking place. \n",
    "\n",
    "For example, let's assume that if I don't find a flamingo, I can still get a picture of one from one of my sources. The probability of me getting a picture of a yellow flamingo is just the sum of the probability of _finding_ a flamingo and getting a picture and _not finding_ a flamingo and getting a picture.\n",
    "\n",
    "The law of total probability enables us to derive another incredibly useful theorem, known as __Bayes' Theorem__, which is used for revising predictions (updating probabilities) given additional evidence. Bayes Theorem is given as follows:\n",
    "\n",
    "$$p(x_{j},y) = \\frac{p(x_{j}, y)}{p_{Y}(y)} =  \\frac{p(y|x_{j})p(x_{j})}{\\sum_{i=1}^{N}p(y|x_{i})p(x_{i})} $$\n",
    "\n",
    "This equation dictates that the probability of the prior outcome $X=x_{j}$ having taken place given that the $Y=y$ has _now_ taken place is given by the ratio between the probability that $X=x_{j}$ and $Y=y$ occured and the probability that $Y=y$ followed any possible event $X=x_{i}$.\n",
    "\n",
    "We will now look at a quick example to understand these concepts in practice, shown in the __tree diagram__ below, with the properties:\n",
    "- Imagine a type of bolt that can be produced either in factory A or factory B. They sometimes end up defective.\n",
    "- 60% of bolts are produced in A and 40% of bolts are produced in B\n",
    "- 2% of bolts produced in A are defective and 4% of bolts produced in B are defective\n",
    "\n",
    "We can model this situation with two random variables:\n",
    "- X: can either take the value 'A' or 'B', corresponding to which factory the bolt was produced in\n",
    "- Y: can either take the value 'D' or 'D'' corresponding to whether it is defective or not\n",
    "\n",
    "<img src=\"images/tree.png\" alt=\"tree-diagram\"\n",
    "\ttitle=\"Tree diagram of the bolt production process\" width=\"750px\" height=\"500px\" />\n",
    "    \n",
    "Given the diagram above and the process, we can answer the following questions:\n",
    "1. What is the probability that the bolt is from factory A and it is defective? <br>\n",
    "$p(A, D) = p(D|A)p_{X}(A) = 0.02\\cdot 0.6 = 0.012 $\n",
    "2. Using the law of total probability, what is the probability that a bolt is defective? <br>\n",
    "$p_{Y}(D) = \\sum_{i=1}^{N}p(D|A_{i})p_{X}(A_{i}) = p(D|A)p_{X}(A) + p(D|B)p_{X}(B) = 0.02\\cdot 0.6 + 0.04\\cdot 0.4 = 0.028 $\n",
    "3. Using Bayes Theorem, what is the probability that a bolt is from factory B, given that it is defective? <br>\n",
    "$p(B|D) = \\frac{p(D|B)p_{X}(B)}{\\sum_{i=1}^{N}p(D|x_{i})p_{X}(A_{i})} = \\frac{0.04\\cdot 0.4}{0.04\\cdot 0.4 + 0.02\\cdot 0.6} = 0.57$\n",
    "4. Using Bayes Theorem, what is the probability that a bolt is from factory A, given that it is defective? <br>\n",
    "$P(A|D) = \\frac{P(D|A)P(A)}{\\sum_{i=1}^{N}P(D|A_{i})P(x_{i})} = \\frac{0.02\\cdot 0.6}{0.04\\cdot 0.4 + 0.02\\cdot 0.6} = 0.43$\n",
    "\n",
    "So with the laws of probability we've gone over, we were able to calculate the probability that a bolt is defective no matter where it came from originally and even what factories are responsible for the majority of the defective bolts, which in this case is factory B!\n",
    "\n",
    "Similarly, if the occurance of an event does not affect the probability of another, implying $P(x|y) = P(x)$, X and Y are __independent random variables__. If I eat a croissant for breakfast today, this will not affect the probability of it raining in two weeks time. Given the above equation, we can conclude that for independent random variables X and Y:\n",
    "$$p(x,y) = p_{X}(x)p_{Y}(y)$$\n",
    "\n",
    "\n",
    "# Probability Chain Rule\n",
    "The __probability chain rule__ is a crucial concept in probability, and is based on the expansion of the concepts we covered in conditional probability so far. It is used for computing the joint probability of any number of random variables based on only their conditional probabilities. We have already looked at the case of two random variables, so for now, let us cover what happens for three random variables X, Y and Z. The probability of $X=x$, $Y=y$ and $Z=z$ taking place can be computed as the probability of $X=x$ _given that_ $Y=y$ and $Z=z$ have taken place times the probability of $Y=y$ and $Z=z$ taking place. This is shown below:\n",
    "\n",
    "$$P(X=x, Y=y, Z=z) = p(x,y,z) = p(x|y,z)p(y,z)$$\n",
    "\n",
    "We can do this by in that particular case, treating the values of \"Y and Z\" one outcome, then applying the relationship for conditional probabilities and joint probabilities we have previously seen:\n",
    "\n",
    "$$p(x,y,z) = p(x|y,z)p(y,z) = p(x|y,z)p(y|z)p_{Z}(z)$$\n",
    "\n",
    "To nail down this concept we will also cover this process for the case of the joint probability of four random variables, X, Y, Z, T. We start by initially treating the values of (Y, Z, T) as one outcome:\n",
    "\n",
    "$$p(x,y,z,t) = p(x|y,z,t)p(y,z,t)$$\n",
    "\n",
    "We now have another joint probability of three random variables, which we can substitute by what we got above:\n",
    "\n",
    "$$p(x,y,z,t) = p(x|y,z,t)p(y,z,t) = p(x|y,z,t)p(y|z,t)p(z|t)p_{T}(t)$$\n",
    "\n",
    "For cases with more than four random variables, we can apply the same logic recursively:\n",
    "- Treat all the values of all but one random variable as one outcome\n",
    "- Re-write the joint probability as a conditional probability times the joint probability of one less variable\n",
    "- Do the same process for the new joint probability until we get to our base case of two random variables\n",
    "\n",
    "We can generalize the above results. For any N random variables $X_{1},X_{2},...,X_{N}$, we can use the chain rule of probability to re-write their joint probability (given the reasoning above) in the following manner:\n",
    "\n",
    "$$p(x_{1},x_{2},...,x_{N}) = \\prod_{i=1}^{N}p(x_{i}|x_{1},...,x_{i-1})$$\n",
    "\n",
    "And that is the gist of it! This is an important tool in data science and machine learning, and is crucial when dealing with Bayesian Networks!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
