{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Content:\n",
    "\n",
    "Part 1:\n",
    "1.   \tTypes of data\n",
    "·       Categorical/Continuous and their descriptive statistics\n",
    "2.   \tMissing values\n",
    "·       Removing rows\n",
    "·       Imputing\n",
    "o   Mean\n",
    "o   Linear regression\n",
    "3.   \tPre-processing\n",
    "·       Datetime\n",
    "·       Regex\n",
    "·       Data normalization/standardization\n",
    "4.   \tUnbalanced data\n",
    "·       Truncating a dataset\n",
    "·       Bootstrapping\n",
    "·       Oversampling vs undersampling\n",
    "o   SMOTE/Tomek Link\n",
    "\n",
    "Part 2:\n",
    "1.   \tVisualising Data\n",
    "·       Ordinal/Nominal/Interval/Ratio Data\n",
    "2.   \tDropping Features/Feature Assumptions\n",
    "·       Relationship between two variables\n",
    "·       Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "__Exploratory Data Anlysis(EDA)__ refers to a set of procedures descriptive and graphical summmaries of your data. This approach allows you to examine the data without any assumptions. It is vital in examining your data, determining the relationship between different variables, and find out if there are any data entry errors. It is primarily used to explain what the data can tell us beyond the usual hypothesis testing task. It was originally proposed to encourage statisticians to explore their data, and possibly create new hypotheses that could lead to more data.\n",
    "\n",
    "As shown below, without EDA tools, we would find it difficult to create models and algorithms to be applied in commercial products or to make important decisions. EDA allows a more robust understanding of the data we have collected.\n",
    "\n",
    "<img src=\"datascienceprocess.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics\n",
    "A __descriptive statistic__ is a summary statistic that quantitatively describes features of a dataset. __Descriptive statistics__ is the process of using and analyzing those statistics. Rather than trying to learn from the data, in descriptive statistics, we simply aim to summarize the data. The most common types of descriptive statistics are measures of __central tendency__ and measures of __dispersion or variability__. We will explore these different measures with a dataset of houses in Brazil and their respective characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6080,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "housing_data = pd.read_csv(\"houses_to_rent.csv\")\n",
    "print(housing_data[\"area\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, when we look at a well-defined collection of objects, known as a __population__, we often do not have access to _all_ members of the population, but may still want to make predictions based on data from the population. In this case, we take a __sample__ from that population, which is a subset of the population we have information on, and estimate the descriptive statistics of the population from the descriptive statistics of the sample.\n",
    "\n",
    "### Measures of Central tendency\n",
    "Central tendency is a central or typical value from the distribution in a set of data points. This measure is based on the idea that data points tend to cluster around a central value.\n",
    "\n",
    "#### Mean\n",
    "The __mean__ of a sample, also known as the __average__, is a quantity used to estimate the mean of the entire population we are looking at, as known as the __expected value__. The sample mean is given by the sum of all observations divided by the number of observations (N):\n",
    "\n",
    "$$\\bar{x} = E(X) =  \\frac{1}{n}\\sum_{i=1}^{N}x_{i} $$\n",
    "\n",
    "Where $E(.)$ represents the __expectation operator__ and X represents the sample.\n",
    "\n",
    "Below we calculate the sample mean of the internal housing area of houses in Brazil. As we do not have access to information for _all_ houses in Brazil, the sample we have will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original sample mean of the internal area of houses in Brazil is 151.14391447368422\n",
      "\n",
      "Rest of the dataset:\n",
      "0       240\n",
      "1        64\n",
      "2       443\n",
      "3        73\n",
      "4        19\n",
      "       ... \n",
      "6075     50\n",
      "6076     84\n",
      "6077     48\n",
      "6078    160\n",
      "6079     60\n",
      "Name: area, Length: 6080, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Computing the sample average\n",
    "housing_data_area = housing_data[\"area\"]\n",
    "area_mean = np.mean(housing_data_area)\n",
    "\n",
    "print(\"The original sample mean of the internal area of houses in Brazil is\",area_mean)\n",
    "print()\n",
    "print(\"Rest of the dataset:\")\n",
    "print(housing_data_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the value we get, it seems to be pretty representative of the dataset. This is the aim of descriptive statistics: to find values that are __representative__ of the dataset. However, the sample mean is not robust in the presence of __outliers__, which are values that are much smaller or much larger that most other observations.\n",
    "\n",
    "Below we introduce a large outlier value to our dataset and recalculate the sample mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new sample mean of the internal area of houses in Brazil is 167.5637230718632\n"
     ]
    }
   ],
   "source": [
    "# Adding outlier to our sample and re-computing the sample mean\n",
    "new_sample = pd.DataFrame({\"area\":[100000]})\n",
    "new_data = housing_data.append(new_sample) # adding a large outlier\n",
    "new_area_mean = np.mean(new_data[\"area\"])\n",
    "                  \n",
    "print(\"The new sample mean of the internal area of houses in Brazil is\",new_area_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, even one large outlier is already enough to change our sample mean. In the presence of outliers, other measures are recommended as a measure of central tendency.\n",
    "\n",
    "#### Median\n",
    "The __median__ of a sample is the middle value of a sequence of observations arranged in ascending order. If we have an odd number of observations, it is the single middle value, and if we have an even number of observations it is the average between the two middle values. The median of internal areas of houses is computed below for both the original housing data and the sample with an outlier introduced. We can see below that the median value of the dataset was not impacted by the outlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original sample median of the internal area of houses in Brazil is 100.0\n",
      "The sample median of the internal area of houses in Brazil with an outlier is 100.0\n"
     ]
    }
   ],
   "source": [
    "# Compute median of both samples\n",
    "original_median = np.median(housing_data_area)\n",
    "new_median = np.median(new_data[\"area\"])\n",
    "\n",
    "print(\"The original sample median of the internal area of houses in Brazil is\",original_median)\n",
    "print(\"The sample median of the internal area of houses in Brazil with an outlier is\",new_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mode\n",
    "In descriptive statistics, we may also want to compute the __mode__ of a sample. This represents the observations with the highest __frequency__, which is the number of times it occurs. It is a useful measure when dealing with categorical data, which we will deal with later. The mode of the internal area of houses in Brazil is computed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original sample mode of the internal area of houses in Brazil is [50]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "\n",
    "# Computing the mode of the original dataset\n",
    "mode_info = mode(housing_data_area)\n",
    "print(\"The original sample mode of the internal area of houses in Brazil is\",mode_info[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measures of Dispersion (Variability)\n",
    "__Dispersion__, also known as __variability__, is a measure of how stretched or squeezed the distribution of our data is. \n",
    "\n",
    "#### Range\n",
    "The __range__ of a sample is the size of the smallest interval in which we can fit _all_ our observations, and can be computed as the difference between the maximum and minimum values out of all observations:\n",
    "\n",
    "$$ range = x_{max} - x_{min} $$\n",
    "\n",
    "Since it only depends on two values of the dataset, it is generally used for smaller datasets. It is also not a robust measure in the presence of outliers. The range of the internal area of houses in Brazil is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample range is 24596\n"
     ]
    }
   ],
   "source": [
    "# Computing the range of the dataset\n",
    "housing_area_range = np.amax(housing_data_area) - np.amin(housing_data_area)\n",
    "print(\"The sample range is\",housing_area_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variance and standard deviation\n",
    "The __variance__ of a population is defined as the average of the square of the difference between the mean of the data and each observation. It is essentially the answer to the question: \"On average, how far are the observations from the mean?\". The square of the difference mainly serves to make sure that all values are positive. The variance of population can be estimated through the __sample variance__, computed as follows:\n",
    "\n",
    "$$s^{2} = E[(X-\\bar{x})^{2}] = \\frac{1}{N-1}\\sum_{i=1}^{N}(x_{i} - \\bar{x})^{2} $$\n",
    "\n",
    "However, since we are taking the square of the differences, if we scale our data by a constant, _a_, the variance of our dataset is scaled by $a^{2}$, meaning that the sample variance is not linear in scale. Thus, what is often used instead of the variance is the square root of the variance, known as the __standard deviation__. A useful property is that, approximately, 68% of the data is within 1 standard deviation from the mean, 95% of the data within 2 standard deviations from the mean and 99.7% of the data is within 3 standard deviations of the mean. \n",
    "\n",
    "Both the standard deviation and variance are computed below for the internal area of houses in Brazil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample variance is 141021.7287952032 and the sample standard deviation is  375.5285991708264\n"
     ]
    }
   ],
   "source": [
    "# Computing variance and standard deviation\n",
    "sample_variance = np.var(housing_data_area)\n",
    "sample_std = np.std(housing_data_area)\n",
    "\n",
    "print(\"The sample variance is\",sample_variance,\"and the sample standard deviation is \",sample_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quartiles and Interquartile Ranges\n",
    "\n",
    "Before we have seen that the median splits our data into separate sections of the same length. Because of how it is defined, we have 50% of the data before the median (first-half) and 50% after (second-half). We can split our dataset further by finding the median of the first-half and the median of the second-half, giving us four distinct subsets of observations, all of equal length. Below is a diagram explaining this process in more detail, where:\n",
    "- Q1 is the median of the first-half, known as the __lower quartile__\n",
    "- Q2 is the median of the full dataset\n",
    "- Q3 is the median of the second-half, known as the __upper quartile__\n",
    "\n",
    "Q1, Q2 and Q3 are the __quartiles__ of the sample.\n",
    "\n",
    "<img src=https://www.onlinemathlearning.com/image-files/xmedian-quartiles.png.pagespeed.ic.fzcCJEohbz.webp />\n",
    "\n",
    "By subtracting Q3 by Q1, we get what is known as the __interquartile range (IQR)__. It is a measure of dispersion that, unlike the _range,_ is unaffected by outliers. Below, we compute the different quartiles and compute a __boxplot__ of the data generated below. We will cover boxplots and other ways of visualizing data in more detail in later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The lower quartile upper-bound (Q1) is 11\n",
      "The median is 15.0\n",
      "The upper quartile lower-bound (Q3) is 23\n",
      "The interquartile range is 12\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD5CAYAAAAgGF4oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAQgUlEQVR4nO3df4xlZX3H8fcHxGJ1kcWdEgosi0YlSAV0BK3EKERFgxWpqGtKUKlrKiRarBFpGiA11VSpaawFl7AVjIhWtFCCtogoEol1FunySyJS1kJWdgjgLtYfZf32j3vWTJf5cWZ2zr07e96v5Obe85xf378+c+Y5z3lOqgpJUn/sMeoCJEnDZfBLUs8Y/JLUMwa/JPWMwS9JPWPwS1LPPKWrAyfZG7gJ+J3mPF+uqvOSHApcCTwLWA+cVlW/nu1YK1asqFWrVnVVqiTtltavX/9wVY3t2N5Z8AO/Ao6vqseT7AXcnORrwNnAJ6vqyiQXA2cAF812oFWrVjExMdFhqZK0+0mycbr2zrp6auDxZnGv5lPA8cCXm/bLgJO7qkGS9GSd9vEn2TPJbcBm4Hrgx8BjVfVEs8kDwIEz7LsmyUSSicnJyS7LlKRe6TT4q2pbVR0FHAQcAxw2j33XVtV4VY2PjT2pi0qStEBDGdVTVY8BNwIvA/ZNsv3ewkHAg8OoQZI00FnwJxlLsm/z+2nAq4G7GfwBeHOz2enA1V3VIEl6si5H9RwAXJZkTwZ/YL5UVdcmuQu4MslHgB8Al3ZYgyRpB50Ff1VtAI6epv0+Bv39kqQR8MldSeqZLrt6pCUnyVDO4wuQNEoGvzTFfAM5iSGuJceuHknqGYNfknrG4JeknjH4JalnDH5J6hmDX5J6xuCXpJ4x+CWpZwx+SeoZg1+Sesbgl6SeMfglqWcMfknqGYNfknrG4JeknjH4JalnDH5J6hmDX5J6xuCXpJ4x+CWpZwx+SeoZg1+Seqaz4E9ycJIbk9yV5M4k72vaz0/yYJLbms/ru6pBkvRkT+nw2E8AH6iqW5MsA9Ynub5Z98mq+kSH55YkzaCz4K+qTcCm5vfWJHcDB3Z1PklSO0Pp40+yCjga+F7TdFaSDUnWJVk+jBokSQOdB3+SZwBXAe+vqi3ARcBzgKMY/Edw4Qz7rUkykWRicnKy6zIlqTc6Df4kezEI/c9X1VcAquqhqtpWVb8BLgGOmW7fqlpbVeNVNT42NtZlmZLUK12O6glwKXB3Vf3dlPYDpmz2JuCOrmqQJD1Zl6N6Xg6cBtye5Lam7VxgdZKjgALuB97TYQ2SpB10OarnZiDTrLquq3NKkubmk7uS1DMGvyT1jMEvST1j8EtSzxj8ktQzBr8k9YzBL0k9Y/BLUs90+eSuNFL77bcfjz76aOfnGcxO0p3ly5fzyCOPdHoO9YvBr93Wo48+SlWNuoyd1vUfFvWPXT2S1DMGvyT1jMEvST1j8EtSzxj8ktQzBr8k9cy8gj/JHkn26aoYSVL35gz+JFck2SfJ0xm8H/euJB/svjRJUhfaXPEfXlVbgJOBrwGHMniXriRpCWoT/Hsl2YtB8F9TVf/bcU2SpA61Cf7PAPcDTwduSnII8LMui5IkdadN8P9rVR1YVa+vwcQnPwHe1XFdkqSOtAn+q6YuNOF/ZTflSJK6NuPsnEkOA14APDPJKVNW7QPs3XVhkqRuzDYt8/OBk4B9gTdMad8KvLvLoiRJ3Zkx+KvqauDqJC+rqlvme+AkBwOXA/sDBaytqr9Psh/wRWAVg5vGb6mq7t+WIUkC2r2I5d4k5zII6t9uX1Vz3eB9AvhAVd2aZBmwPsn1wDuAG6rqY0nOAc4BPrSQ4iVJ89cm+K8GvgN8A9jW9sBVtQnY1PzemuRu4EDgjcArm80uA76FwS9JQ9Mm+H+3qnYqmJOsAo4Gvgfs3/xRAPgpg64gSdKQtBnOeW2S1y/0BEmewWBI6PubqR9+qxkaOu1LUZOsSTKRZGJycnKhp5ck7aBN8L+PQfj/IsmWJFuTbJlzL6CZ6uEq4PNV9ZWm+aEkBzTrDwA2T7dvVa2tqvGqGh8bG2tzOklSC3MGf1Utq6o9quppVbVPszzn1MxJAlwK3F1Vfzdl1TXA6c3v0xncQ5AkDcmsD3BV1Q+TvGi69VV16xzHfjmDWTxvT3Jb03Yu8DHgS0nOADYCb5l/2ZKkhZrt5u7ZwBrgwmnWFXD8bAeuqpuBzLD6hFbVSZIW3WwPcK1pvl81vHIkSV2bczhnc4P2z4BXNE3fAj7jvPyStDS1Gcd/EbAX8I/N8mlN2592VZQkqTttgv8lVXXklOVvJvnPrgqSJHWrzTj+bUmes30hybOZx9QNkqRdS5sr/g8CNya5j8EonUOAd3ZalSSpM3MGf1XdkOS5DObnB7inqn7VbVmSpK60GdWzN/Be4DgG4/e/k+Tiqvpl18VJkhZfm66eyxm8detTzfLbgc8Bp3ZVlCSpO22C/4iqOnzK8o1J7uqqIElSt9qM6rk1yUu3LyQ5FpjoriRJUpfaXPG/GPhukp80yyuBe5LczmBK/Rd2Vp0kadG1Cf4TO69CkjQ0bYZzbhxGIZKk4WjTxy9J2o0Y/JLUMwa/JPXMnMGf5KVJvp/k8SS/TrKt7cvWJUm7njZX/P8ArAZ+BDyNwTz8n+6yKElSd1p19VTVvcCeVbWtqv4Jh3hK0pLVZhz//yR5KnBbkr8FNuG9AUlastoE+GnNdmcBPwcOBk7psihJUnfaBP/JVfXLqtpSVRdU1dnASV0XJknqRpvgP32atncsch2SpCGZsY8/yWoGc+8fmuSaKauWAY90XZgkqRuz3dz9LoMbuSuAC6e0bwU2zHXgJOsYdAltrqojmrbzgXcDk81m51bVdfMvW5K0UDMGfzM520bgZQs89mcZPANw+Q7tn6yqTyzwmJKkndTZk7tVdRN2CUnSLmcUT+6elWRDknVJlu/EcSRJC5Cqmn2DZKKqxpNs2P62rSQ/qKqj5zx4sgq4dkof//7Aw0ABfw0cUFXvmmHfNcAagJUrV75440ZfC6B5Ov+Zo65g8Zz/s1FXoCUoyfqqGt+xfahP7lbVQ1MKugS4dpZt1wJrAcbHx2f/6yRNIxdsYa4Lm6UgCXX+qKvQ7qTtk7t78v+f3P3jhZwsyQFTFt8E3LGQ40iSFm4+r178BXBB2wMn+QLwSmBFkgeA84BXJjmKQVfP/cB75lmvJGknzfYA1+0MAnpa2/v7Z1m/eprmS9uXJknqwmxX/Nvn4zmz+f5c8/0nzPIHQZK0a5vrAS6SvHqHETwfSnIrcE7XxUmSFl+bm7tJ8vIpC3/Ycj9J0i6ozXDOM4B1SbYPin4MmHbsvSRp19dmVM964MjtwV9VPkkiSUtYmyt+wMCXpN2FffWS1DMGvyT1TJtpmdcnOdOZNCVp99Dmiv+twO8D309yZZLXJknHdUmSOjJn8FfVvVX1l8DzgCuAdcDGJBck2a/rAiVJi6tVH3+SFzJ47+7HgauAU4EtwDe7K02S1IU5h3MmWc/goa1LgXOq6lfNqu9NfaJXkrQ0zBr8SfYArqqqv5lufVWd0klVkqTOzNrVU1W/AQx3SdqNtOnj/0aSv0hycJL9tn86r0yS1Ik2Uza8tfk+c0pbAc9e/HIkSV1rM0nbocMoRJI0HK0maUtyBHA4sPf2tqq6vKuiJEndaTOc8zwGL00/HLgOeB1wM2DwS9IS1Obm7puBE4CfVtU7gSOBZ86+iyRpV9Um+H/RDOt8Isk+wGbg4G7LkiR1pU0f/0SSfYFLgPXA48AtnVYlSerMXE/uBvhoVT0GXJzk68A+VbVhKNVJkhbdrMFfVZXkOuAPmuX7h1GUJKk7bfr4b03yks4rkSQNRZvgPxa4JcmPk2xIcnuSObt6kqxLsjnJHVPa9ktyfZIfNd++1UuShqxN8L8WeA5wPPAG4KTmey6fBU7coe0c4Iaqei5wQ7MsSRqiNsH/karaOPUDfGSunarqJuCRHZrfCFzW/L4MOHle1UqSdlqb4H/B1IUkewIvXuD59q+qTc3vnwL7z7RhkjVJJpJMTE5OLvB0kqQdzRj8ST6cZCvwwiRbms9WBg9wXb2zJ66qYjDL50zr11bVeFWNj42N7ezpJEmNGYO/qj5aVcuAj1fVPs1nWVU9q6o+vMDzPZTkAIDme/MCjyNJWqDZrvgPa37+c5IX7fhZ4PmuAU5vfp/OIvznIEman9ke4DobWANcOM26YjDKZ0ZJvsBgVs8VSR4AzgM+BnwpyRnARuAtC6hZkrQTZgz+qlrTfL9qIQeuqtUzrDphIceTJC2ONvPx7w28FziOwZX+d4CLq+qXHdcmSepAm9k5Lwe2Ap9qlt8OfA44tauipMUymGdwaVu+3AfctbjaBP8RVXX4lOUbk9zVVUHSYhmMGO5WkqGcR1pMbSdpe+n2hSTHAhPdlSRJ6tKMV/xJbmfQp78X8N0kP2mWDwF+OJzyJEmLbbaunpOGVoUkaWhmG865sZmX586qOmym7SRJS8usffxVtQ24J8nKIdUjSepYm1E9y4E7k/wH8PPtjVX1R51VJUnqTJvg/6vOq5AkDc2cwV9V3566nOQ4YDXw7en3kCTtytpc8ZPkaAZP7J4K/BdwVZdFSZK6M9s4/ucxuLJfDTwMfBHIQidtkyTtGma74v8hgwnZTqqqewGS/PlQqpIkdWa24ZynAJsYzM1zSZITgKU/45Uk9dxsr178l6p6G3AYcCPwfuD3klyU5DXDKlCStLjmnKStqn5eVVdU1RuAg4AfAB/qvDJJUifazM75W1X1aFWtrSrfoiVJS9S8gl+StPQZ/JLUMwa/JPWMwS9JPWPwS1LPGPyS1DMGvyT1TKvZORdbkvuBrcA24ImqGh9FHZLURyMJ/sarqurhEZ5fknrJrh5J6plRBX8B/55kfZI1022QZE2SiSQTk5OTQy5PknZfowr+46rqRcDrgDOTvGLHDZo5gcaranxsbGz4FUrSbmokwV9VDzbfm4GvAseMog5J6qOhB3+SpydZtv038BrgjmHXIUl9NYpRPfsDX02y/fxXVNXXR1CHJPXS0IO/qu4Djhz2eSVJAw7nlKSeMfglqWcMfknqGYNfknrG4JeknjH4JalnDH5J6hmDX5J6xuCXpJ4x+CWpZwx+SeoZg1+Sesbgl6SeMfglqWcMfknqGYNfknrG4JeknjH4JalnDH5J6hmDX5J6xuCXpJ4x+CWpZwx+SeoZg1+Sesbgl6SeGUnwJzkxyT1J7k1yzihqkKS+GnrwJ9kT+DTwOuBwYHWSw4ddhyT11Siu+I8B7q2q+6rq18CVwBtHUIck9dJTRnDOA4H/nrL8AHDsjhslWQOsAVi5cuVwKlPvJRnKPlU1732kxbLL3tytqrVVNV5V42NjY6MuRz1RVUP5SKM0iuB/EDh4yvJBTZskaQhGEfzfB56b5NAkTwXeBlwzgjokqZeG3sdfVU8kOQv4N2BPYF1V3TnsOiSpr0Zxc5equg64bhTnlqS+22Vv7kqSumHwS1LPGPyS1DMGvyT1TJbCwyRJJoGNo65DmsYK4OFRFyHN4JCqetITsEsi+KVdVZKJqhofdR3SfNjVI0k9Y/BLUs8Y/NLOWTvqAqT5so9fknrGK35J6hmDX5J6xuCXFiDJuiSbk9wx6lqk+TL4pYX5LHDiqIuQFsLglxagqm4CHhl1HdJCGPyS1DMGvyT1jMEvST1j8EtSzxj80gIk+QJwC/D8JA8kOWPUNUltOWWDJPWMV/yS1DMGvyT1jMEvST1j8EtSzxj8ktQzBr8k9YzBL0k9839Fc5aaNJs4lwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generating data\n",
    "data = np.random.randint(0,30,45) # generating 45 observations of integers between 0 and 30\n",
    "\n",
    "# Computing Q1 and Q3\n",
    "data = np.sort(data) # sorting our data into ascending order\n",
    "Q2 = np.median(data) # median of our data\n",
    "Q1 = data[12] # lower quartile\n",
    "Q3 = data[34] # upper quartile\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Displaying Quartile Statistics\n",
    "print(\"The lower quartile upper-bound (Q1) is\",Q1)\n",
    "print(\"The median is\",Q2)\n",
    "print(\"The upper quartile lower-bound (Q3) is\",Q3)\n",
    "print(\"The interquartile range is\",IQR)\n",
    "\n",
    "# Creating boxplot for our data\n",
    "plt.figure()\n",
    "plt.boxplot(data)\n",
    "plt.ylabel(\"Arbitrary data points\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Data\n",
    "In the field of exploratory data analysis, one key piece information about the data is the __type__ of data you are analysing. The type of data being investigated determines what statistical tools are available/recommended, and understanding the different types is vital in effectively analysing different datasets.\n",
    "\n",
    "### Categorical Data\n",
    "__Categorical data__ refers to data that has a _finite_ number of possible categories we can observe. If we are collecting data on someone's country of births, we know that there are only 195 possibilites. Likewise, if we are collecting data on the highest place each player reached in a US Open championship, there are only 156 possible places. Both of these are examples of categorical data.\n",
    "\n",
    "In the case of country of birth being collected as data, there is no inherent ordering. This type of data is known as __nominal data__, and is the classification of data such as name, gender and ethnicity. \n",
    "\n",
    "On the other hand, we know that a player that reached 1st place in the US Open scored higher than those in 2nd or 3rd place, so there is an inherent ordering of our data. This type of data is known as __ordinal data__, and refers to data that has an order, but the distance between possibilities is not uniform throughout. The difference between 1st and 2nd place is not necessarily the same as the difference between 2nd and 3rd, 3rd and 4th, and so on. Given this, ordinal scales can be used to measure non-numeric features like happiness, customer satisfaction or even the  difficulty of ski slopes!\n",
    "\n",
    "When dealing with categorical data, we can calculate how many times a category occurs, known as the __frequency__. We can also determine the __relative frequency__ by dividng the frequency of a category by the total number of observations. Using frequencies, we can calculate the _mode_ of the data set. Given that nominal data is not numerical and has no inherent order, we cannot compute any other statistics on it. With ordinal data, on the other hand, given the inherent ordering, we can also calculate the _median,_ and consequently, the _interquartile range._ We will view how to visualize these data types in a later section.\n",
    "\n",
    "We will now compute the descriptive statistics of the \"floor\" feature, which is ordinal, and the \"furniture\" feature, which is nominal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>city</th>\n",
       "      <th>area</th>\n",
       "      <th>rooms</th>\n",
       "      <th>bathroom</th>\n",
       "      <th>parking spaces</th>\n",
       "      <th>floor</th>\n",
       "      <th>animal</th>\n",
       "      <th>furniture</th>\n",
       "      <th>hoa</th>\n",
       "      <th>rent amount</th>\n",
       "      <th>property tax</th>\n",
       "      <th>fire insurance</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>240</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>-</td>\n",
       "      <td>acept</td>\n",
       "      <td>furnished</td>\n",
       "      <td>R$0</td>\n",
       "      <td>R$8,000</td>\n",
       "      <td>R$1,000</td>\n",
       "      <td>R$121</td>\n",
       "      <td>R$9,121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>acept</td>\n",
       "      <td>not furnished</td>\n",
       "      <td>R$540</td>\n",
       "      <td>R$820</td>\n",
       "      <td>R$122</td>\n",
       "      <td>R$11</td>\n",
       "      <td>R$1,493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>443</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>acept</td>\n",
       "      <td>furnished</td>\n",
       "      <td>R$4,172</td>\n",
       "      <td>R$7,000</td>\n",
       "      <td>R$1,417</td>\n",
       "      <td>R$89</td>\n",
       "      <td>R$12,680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>acept</td>\n",
       "      <td>not furnished</td>\n",
       "      <td>R$700</td>\n",
       "      <td>R$1,250</td>\n",
       "      <td>R$150</td>\n",
       "      <td>R$16</td>\n",
       "      <td>R$2,116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>not acept</td>\n",
       "      <td>not furnished</td>\n",
       "      <td>R$0</td>\n",
       "      <td>R$1,200</td>\n",
       "      <td>R$41</td>\n",
       "      <td>R$16</td>\n",
       "      <td>R$1,257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6075</th>\n",
       "      <td>6075</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>acept</td>\n",
       "      <td>not furnished</td>\n",
       "      <td>R$420</td>\n",
       "      <td>R$1,150</td>\n",
       "      <td>R$0</td>\n",
       "      <td>R$15</td>\n",
       "      <td>R$1,585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6076</th>\n",
       "      <td>6076</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>not acept</td>\n",
       "      <td>furnished</td>\n",
       "      <td>R$768</td>\n",
       "      <td>R$2,900</td>\n",
       "      <td>R$63</td>\n",
       "      <td>R$37</td>\n",
       "      <td>R$3,768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6077</th>\n",
       "      <td>6077</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>acept</td>\n",
       "      <td>not furnished</td>\n",
       "      <td>R$250</td>\n",
       "      <td>R$950</td>\n",
       "      <td>R$42</td>\n",
       "      <td>R$13</td>\n",
       "      <td>R$1,255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>6078</td>\n",
       "      <td>1</td>\n",
       "      <td>160</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-</td>\n",
       "      <td>not acept</td>\n",
       "      <td>not furnished</td>\n",
       "      <td>R$0</td>\n",
       "      <td>R$3,500</td>\n",
       "      <td>R$250</td>\n",
       "      <td>R$53</td>\n",
       "      <td>R$3,803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6079</th>\n",
       "      <td>6079</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>acept</td>\n",
       "      <td>furnished</td>\n",
       "      <td>R$489</td>\n",
       "      <td>R$1,900</td>\n",
       "      <td>R$0</td>\n",
       "      <td>R$25</td>\n",
       "      <td>R$2,414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6080 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  city  area  rooms  bathroom  parking spaces floor  \\\n",
       "0              0     1   240      3         3               4     -   \n",
       "1              1     0    64      2         1               1    10   \n",
       "2              2     1   443      5         5               4     3   \n",
       "3              3     1    73      2         2               1    12   \n",
       "4              4     1    19      1         1               0     -   \n",
       "...          ...   ...   ...    ...       ...             ...   ...   \n",
       "6075        6075     1    50      2         1               1     2   \n",
       "6076        6076     1    84      2         2               1    16   \n",
       "6077        6077     0    48      1         1               0    13   \n",
       "6078        6078     1   160      3         2               2     -   \n",
       "6079        6079     1    60      2         1               1     4   \n",
       "\n",
       "         animal      furniture      hoa rent amount property tax  \\\n",
       "0         acept      furnished      R$0     R$8,000      R$1,000   \n",
       "1         acept  not furnished    R$540       R$820        R$122   \n",
       "2         acept      furnished  R$4,172     R$7,000      R$1,417   \n",
       "3         acept  not furnished    R$700     R$1,250        R$150   \n",
       "4     not acept  not furnished      R$0     R$1,200         R$41   \n",
       "...         ...            ...      ...         ...          ...   \n",
       "6075      acept  not furnished    R$420     R$1,150          R$0   \n",
       "6076  not acept      furnished    R$768     R$2,900         R$63   \n",
       "6077      acept  not furnished    R$250       R$950         R$42   \n",
       "6078  not acept  not furnished      R$0     R$3,500        R$250   \n",
       "6079      acept      furnished    R$489     R$1,900          R$0   \n",
       "\n",
       "     fire insurance     total  \n",
       "0             R$121   R$9,121  \n",
       "1              R$11   R$1,493  \n",
       "2              R$89  R$12,680  \n",
       "3              R$16   R$2,116  \n",
       "4              R$16   R$1,257  \n",
       "...             ...       ...  \n",
       "6075           R$15   R$1,585  \n",
       "6076           R$37   R$3,768  \n",
       "6077           R$13   R$1,255  \n",
       "6078           R$53   R$3,803  \n",
       "6079           R$25   R$2,414  \n",
       "\n",
       "[6080 rows x 14 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For ordinal data: frequency(3rd floor) = 401 , relative frequency(3rd floor) = 0.08861878453038674\n",
      "Mode floor number is: [1]\n",
      "Median floor number is: 6.0\n",
      "The interquartile range of floor numbers is: 8.0\n",
      "\n",
      "For nominal data: frequency(furnished) = 1582 , relative frequency(furnished) = 0.2601973684210526\n",
      "Mode furniture category: ['not furnished']\n"
     ]
    }
   ],
   "source": [
    "# Function that returns the frequency of a given category in a dataset\n",
    "def freq(sample,category):\n",
    "    freq = 0\n",
    "    for observation in sample:\n",
    "        if observation == category:\n",
    "            freq+=1\n",
    "    return freq # return frequency of the category\n",
    "\n",
    "\n",
    "# Computing statistics of floor variable\n",
    "floors = housing_data[\"floor\"]\n",
    "floors = [int(floor) for floor in floors if floor != \"-\"] # remove all dashes from our list\n",
    "\n",
    "three_floors = freq(floors,3)\n",
    "relative_freq_floors = three_floors/len(floors)\n",
    "floors_mode = mode(floors)\n",
    "# Calculating Q1, Q2(median), Q3 and IQR\n",
    "quartiles = np.percentile(floors,[25,50,75])\n",
    "IQR = quartiles[2] - quartiles[0]\n",
    " \n",
    "\n",
    "# Computing statistics of furniture variable, which is either furnished or not furnished\n",
    "furnished = housing_data[\"furniture\"]\n",
    "furnished_freq = freq(furnished,\"furnished\")\n",
    "rel_freq = furnished_freq/len(furnished)\n",
    "furnished_mode = mode(furnished)\n",
    "\n",
    "# Displaying results\n",
    "print(\"For ordinal data: frequency(3rd floor) =\",three_floors, \", relative frequency(3rd floor) =\",relative_freq_floors)\n",
    "print(\"Mode floor number is:\",floors_mode[0])\n",
    "print(\"Median floor number is:\",quartiles[1])\n",
    "print(\"The interquartile range of floor numbers is:\",IQR)\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"For nominal data: frequency(furnished) =\",furnished_freq, \", relative frequency(furnished) =\",rel_freq)\n",
    "print(\"Mode furniture category:\",furnished_mode[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical Data\n",
    "__Numerical data__ refers to data that we can carry out different operations on, and can either be __discrete__ or __continuous__. The common distinction is that continuous data is something you __measure__ (weight, height, temperature) and discrete data is something you __count__ (population, number of candies in a bag). Discrete data contains separate values, whereas continuous data can take any value in a given range. For example, if I am measuring how many US Opens a tennis player has won in their career, the number can only be 0,1,2,3,4,5,6 or 7, but nothing in between, having then a _finite_ set of possibilities, being a discrete variable. If am measuring the height of previous US Open winners, even if with the range 150-215cm, there are _infinitely_ many possible heights that can be observed, making it a continuous variable.\n",
    "\n",
    "Numerical data can be further sub-divided into __interval data__ and __ratio data__. Interval values represent ordered values that have the same units, meaning that unlike ordinal data, we know the difference between different values. However, interval data does not have an __absolute zero__. This means that we can add or subtract this data, but we cannot multiply or divide it. An example of interval data is temperature. Temperature has no absolute zero, as in there no such concept as 'no temperature'. This means that I can say that 32ºC is 16ºC warmer than 16ºC, but I CANNOT say that 32ºC is twice as warm as 16ºC.\n",
    "\n",
    "On the other hand, ratio data has an absolute zero. For instance, height is a type of ratio data. Since 0cm is the absolute zero, we can add, subtract, multiply and divide this data type. Given this, we can make statements such as \"2 metres is twice as tall as 1 metre\".\n",
    "\n",
    "For numerical data, we can apply all the descriptive statistics we have covered so far! We apply these below to the \"total\" price sample, which is a type of ratio data given it has an absolute zero. We will review ways to visualize these data types in later sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Central tendency:\n",
      "Mean: 6033.194572368421 , Median: 4128.5 , Mode: [2555]\n",
      "\n",
      "Dispersion:\n",
      "Variance: 76573490.1702008 , Standard deviation: 8750.62798719045 , Range: 372040 , Interquartile range: 5622.5\n"
     ]
    }
   ],
   "source": [
    "# Computing the descriptive statistics\n",
    "\n",
    "# cleaning our data to compute the statistics (removing non-numerics from the string)\n",
    "total_price = housing_data[\"total\"]\n",
    "number_price = []\n",
    "for element in total_price:\n",
    "    new_price = filter(str.isdigit, element)\n",
    "    new_price = \"\".join(new_price)\n",
    "    number_price.append(int(new_price))\n",
    "\n",
    "    \n",
    "# Central tendency statistics and quartiles\n",
    "mean_price = np.mean(number_price)\n",
    "mode_price = mode(number_price)\n",
    "quartiles = np.percentile(number_price,[25,50,75])\n",
    "\n",
    "# Measures of dispersion\n",
    "variance = np.var(number_price)\n",
    "stdev = np.std(number_price)\n",
    "price_range = np.amax(number_price) - np.amin(number_price) # max - min\n",
    "IQR = quartiles[2] - quartiles[0] # Q3 - Q1\n",
    "\n",
    "print(\"Central tendency:\")\n",
    "print(\"Mean:\",mean_price,\", Median:\",quartiles[1],\", Mode:\",mode_price[0])\n",
    "print()\n",
    "print(\"Dispersion:\")\n",
    "print(\"Variance:\",variance,\", Standard deviation:\",stdev,\", Range:\",price_range,\", Interquartile range:\",IQR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values (examples will be added from web scraping)\n",
    "In the real-world, data is never so clean as to have all its values filled in for all features. Sometimes, we have to deal with __missing data__, where for some observation, one or more feature values may be missing. Not all data goes missing for the same reason. Thus, to account for the missing data appropriately, we classify it under three categories:\n",
    "- __Missing Completely at Random(MCAR):__ implies that the data goes missing at a consistent rate independently of feature variables \n",
    "- __Missing at Random(MAR):__ implies that the data goes missing at a consistent rate, but is dependent on other variable and must be determined through other pieces of information from our data\n",
    "- __Not Missing at Random(NMAR):__ implies the probability of a value being missing is dependent on the true value itself\n",
    "\n",
    "\n",
    "\n",
    "# INTRODUCE THE DATASET HERE (Maybe add tables too?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce the dataset we would want here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row Removal\n",
    "The simplest method of dealing with missing values in our data is known as __row removal__, which means we remove any rows with missing features. This approach is especially recommended if _most_ feature values of an observation are missing and if we have a large dataset. However, if the missing data is not MCAR, the row removal method might end up disproportionally removing observations of different features, resulting in __bias__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row removal example with the given dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mean Imputation\n",
    "Rather than removing examples from our dataset, we can __impute__ the missing values, meaning we create replacement values based on the rest of our data.\n",
    "\n",
    "One way this is done is to take the mean of all other non-missing values of the same feature from other examples. It is a simple way of predicting _appropriate_ values to be set for a given missing value so that the example can be used. However, if there are many missing values for the same feature, often the case for missing data that is not MCAR, this approach will reduce the variance of that feature for your dataset, which is not necessarily desirable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature mean imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling Imputation\n",
    "Another method of missing value imputation is by using a machine learning algorithm. We first split our data so that all examples with non-missing values are in the training set, and all examples with the same feature values missing are in the test data set. We then use our trained model to predict the missing value of the examples in our test set. One of the models we have already investigated is a _linear regression model_. By assuming a linear relatinship between features, we can impute missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling imputation example with the given dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "- Definition\n",
    "- Discuss importance of EDA in pre-processing your data\n",
    "    - Dealing with outliers (Should I cover simple methods of dealing with outliers?)\n",
    "        - Tukey IQR\n",
    "        - Kernel Density Estimation\n",
    "- Discuss the importance of handling missing data, which is what we have just covered\n",
    "- Dealing with datatypes, initially categorical (but leads to datetime and regex types)\n",
    "\n",
    "### Datetime\n",
    "- Dates and/or times can be manipulated via the datetime module\n",
    "- Importance of datetimes in handling different date information for preprocessing\n",
    "- naive vs non-naive datetimes\n",
    "- datetime.date(year,month,day)   (No leading zeros in the inputs!)\n",
    "    - Naive date based on gregorian calendar with no time\n",
    "    - datetime.date.today()   (today's date)\n",
    "- datetime.time(hour,minute,second,microsecond)  (no leading zeros in the inputs!)\n",
    "- datetime.datetime(year,month,day,hour,minute,second,microsecond)\n",
    "- datetime.timedelta(year,month,day,hour,minute,second,microsecond)     \n",
    "    - Difference between two datetimes\n",
    "- Different attributes of these data types\n",
    "    - .year\n",
    "    - .month\n",
    "    - .day\n",
    "    - .hour\n",
    "    - .minute\n",
    "    - .second\n",
    "    - .microsecond\n",
    "- Datetimes and timedeltas can be added or subtracted from datetime objects (with + or -)\n",
    "    - datetime +or- timedelta = datetime\n",
    "    - datetime +or- datetime = timedelta\n",
    "- .today() and .now() methods!!\n",
    "- Briefly introduce pytz module for timezone data (datetime.tzinfo())\n",
    "- .strftime(format), returns string corresponding to appropriate datetime object in the given format\n",
    "- datetime.datetime.strptime(string,format) converts string in a given format into a datetime object\n",
    "\n",
    "- Exercise: given a list of different dates in a text file (same format), write a function that finds the most recent date out of all dates and one function that finds the date furtherst in the past using datetime\n",
    "\n",
    "\n",
    "### Regex\n",
    "- \"A regex pattern is a special language used to represent generic text, numbers or symbols so it can be used to extract texts that match that pattern\"\n",
    "- Regular expressions in Python using re module (also known as Regexes)\n",
    "- Allows us to match and search for specific patterns of text (we can make patterns of any text you think of)\n",
    "- Mention the relevance of regex and regular expression data types in NLP, which is covered later\n",
    "- Helpful info about strings for using _re_\n",
    "    - triple quotation marks for multi-line string manipulation\n",
    "    - Python raw strings (prefixed with an r)\n",
    "- re.compile(pattern) to generate regular expression type from a string (generally a raw string)\n",
    "- .finditer(string text) \n",
    "    - returns iterator that contains all matches of your re type in the input text and their location\n",
    "    - case sensitive\n",
    "    - order sensitive\n",
    "- .search()\n",
    "    - scan through an input string, looking for any location where this RE matche\n",
    "- .match()\n",
    "    - Determine if the RE matches at the beginning of the string.\n",
    "- .findall()\n",
    "    - Find all substrings where the RE matches, and returns them as a list\n",
    "    \n",
    "- Escaping special characters with a backslash to search for them (\\. instead of .)\n",
    "\n",
    "- Special sequences:\n",
    ".       - Any Character Except New Line\n",
    "\\d      - Digit (0-9)\n",
    "\\D      - Not a Digit (0-9)\n",
    "\\w      - Word Character (a-z, A-Z, 0-9, _)\n",
    "\\W      - Not a Word Character\n",
    "\\s      - Whitespace (space, tab, newline)\n",
    "\\S      - Not Whitespace (space, tab, newline)\n",
    "\n",
    "\\b      - Word Boundary\n",
    "\\B      - Not a Word Boundary\n",
    "^       - Beginning of a String\n",
    "$       - End of a String\n",
    "\n",
    "Q\n",
    "[]      - Matches Characters in brackets\n",
    "[^ ]    - Matches Characters NOT in brackets\n",
    "|       - Either Or\n",
    "( )     - Group (similar to parenthesis in mathematical expressions)\n",
    "\n",
    "    \n",
    "- Qualifiers:\n",
    "*       - 0 or More\n",
    "+       - 1 or More\n",
    "?       - 0 or One\n",
    "{3}     - Exact Number\n",
    "{3,4}   - Range of Numbers (Minimum, Maximum)\n",
    "    \n",
    "    \n",
    "- Useful for parsing information from strings in a more general form\n",
    "- Exercise example: Out of a list of URLs, some written in different formats than other, extract only the domain name\n",
    "\n",
    "\n",
    "### Feature Scaling\n",
    "- Values in raw data vary widely, and often different features are not on the same scale\n",
    "-  Many classifiers calculate the distance between two points by the Euclidean distance. If one of the featurehas a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized or standardized so that each feature contributes approximately proportionately to the final distance.\n",
    "- Provide example of what happens without normalization/standardization\n",
    "Goal is to ensure that the values of all features vary between 0-1, ensuring all values are on the same scale\n",
    "\n",
    "- Normalization\n",
    "    - Formal definition\n",
    "    - $x' = \\frac{x - x_{min}}{x_{max} + x_{min}}$ (min-max normalization)\n",
    "    \n",
    "- Standardization (Z-Score normalization)\n",
    "    - Formal definition\n",
    "    - $x' = \\frac{x - \\bar{x}}{\\sigma}$\n",
    "\n",
    "- Exercise: Apply normalization and standardization to the previous example where the lack of it caused errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unbalanced Data\n",
    "\n",
    "### Truncating a dataset\n",
    "\n",
    "\n",
    "### Bootstrapping\n",
    "\n",
    "\n",
    "### Oversampling versus undersampling\n",
    "\n",
    "#### SMOTE/Tomek Link\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
