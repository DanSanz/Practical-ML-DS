{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Hypothesis Testing\n",
    "\n",
    "### Learning Objectives:\n",
    "- [Introduction: Statistical Significance & Probability](#Introduction:-Statistical-Significance-&-Probability)\n",
    "- [Normal Distributions, Standardization & Z-tests](#Normal-Distributions,-Standardization-&-Z-tests)\n",
    "- [T-tests](#T\\-tests)\n",
    "- [ANOVA Testing](#Anova-Testing)\n",
    "- [Chi-squared Testing](#Chi\\-squared-Testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: Statistical Significance & Probability\n",
    "\n",
    "Having covered the motivation behind statistical hypothesis testing, we will now look at the mechanisms behind this framework so that you can understand how to be able to carry out your own hypothesis tests! We have seen that the aim of hypothesis testing is to help different scientists to reach the same conclusion with the same data. But how to we bring together the ideas of _objectiveness_ and _randomness_?\n",
    "\n",
    "This is where the probability and probability distributions we have encountered come in handy. Since _probability_ is a measure of how likely something is to occur, and _probability distributions_ tell us the probability/probability density of every possible outcome, we can use probability distributions to tell us the __probability of something being true given our null hypothesis__. This means that if the probability of some __test statistic__, which is based on our data given the null hypothesis is true is small _enough,_ we can reject the null hypothesis and accept the alternative \n",
    "hypothesis. This probability is known as a __p-value__, and the _threshold_ of probability for rejecting the null hypothesis is known as the __significance level__ ($\\mathbf{\\alpha}$), which is determined depending on how certain we need to be. If the p-value we compute from our data is lesser than our chosen significance level, our results are __statistically significant__ and we can reject the null hypothesis.\n",
    "\n",
    "Therefore, we can formalize the steps of hypothesis testing:\n",
    "1. Formulate a hypothesis \n",
    "2. Find the appropriate statistical test\n",
    "3. Choose a significance level\n",
    "4. Collect data and compute test statistic\n",
    "5. Determine the p-value (probability)\n",
    "6. Reject/accept null hypothesis\n",
    "7. Make a decision\n",
    "\n",
    "We will understand these steps in the context of four widely used statistical tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Distributions, Standardization & Z-tests\n",
    "\n",
    "As we have seen before, the normal distribution is a bell-shaped curve centered around the mean, whose width is determined by its standard deviation. It is a useful distribution, as we can approximate many random variables to have a normal distribution, and the _central limit theorem_ we previously encountered also allows us to make useful approximations. We will now explore the normal distribution in relation to hypothesis testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to hypothesis testing, we often use the normal distribution to give us measures of probability. However, we tend to simplify the normal distribution to only have the necessary components for testing. So, what parts of the normal distribution actually contribute to the probability we compute? The first thing we need to understand is that the location of the mean is irrelevant. __We only care about the displacement away from the mean__. Secondly, we see from the above plots that the standard deviation affects the probability of a given displacement from the mean, meaning that __the probability depends on the displacement away from the mean proportional to the standard deviation.__\n",
    "\n",
    "This gives us motivation for an incredibly powerful tool, known as __standardization__. The goal of standardization is to compare any normally distributed data to the __standard normal distribution__, which is a normal distribution with a population mean $\\mu = 0$ and a population standard deviation $\\sigma = 1$. This is useful since we don't need the explicit mean and standard deviation, but rather _how many standard deviations away from the mean a value is_ to calculate its probability.\n",
    "\n",
    "To standardize a dataset, we carry out the following two steps:\n",
    "- Subtract the mean from all points in our dataset to ensure it is centered at 0\n",
    "- Divide the result by the standard deviation of the dataset, scaling the results so that the standard deviation is 1\n",
    "\n",
    "For each data value we standardize, we denote it as shown below:\n",
    "\n",
    "$$z = \\frac{x-\\mu}{\\sigma}$$\n",
    "\n",
    "Where $x$ is our data point, referred to as a __raw score__, and $z$ is known as the __z-score__ or __standard score__. By definition, it is a measure of how many standard deviations a data point is from the mean.\n",
    "\n",
    "Why do we bother using the standard normal distribution? Couldn't we get our probabilities from the _raw_ distributions? In practice, you _could_ just use the original normal distribution, even in a statistical test. But by using z-scores, we can use _one_ hollistic distribution to describe the probabilities of _any_ normally distributed datasets. Another reason is that since the to get a probability from a pdf, we have to find the area under its curve within a given range, we would to carry out complex operations for each distribution. Instead, statisticians have created a __z-table__, which tells us the _cumulative probability_ of any given z-score. We can use this to compute probabilities for any given normal distribution! While this is not _as_ relevant as when we couldn't use special calculators and computers to give us these probabilities, it still greatly simplifies the process.\n",
    "\n",
    "Let us consider the example that we covered in the Prelude week for the height of students, denoted by $X$, where $X \\sim N(175, 4^{2})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T-tests\n",
    "https://blog.minitab.com/blog/michelle-paret/guinness-t-tests-and-proving-a-pint-really-does-taste-better-in-ireland"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANOVA Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chi-squared Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5780000000000001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.645-1.067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
