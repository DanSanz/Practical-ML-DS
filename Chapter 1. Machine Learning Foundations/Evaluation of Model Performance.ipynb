{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Model Performance\n",
    "\n",
    "### Learning Objectives:\n",
    "- [Evaluating Models With Test Sets](#Evaluating-Models-With-Test-Sets)\n",
    "- [K-fold Cross Validation](#K\\-fold-Cross-Validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Models With Test Sets\n",
    "Now you have looked at different types of models. But how do we evaluate their performance? What would happen if we trained our model with the input data, then tested the performance of our model on the data we used to train it? This is not ideal, as the model has trained to fit that particular data. We are more interested in the ability of a model to make accurate predictions on data is has never seen before, or in other words, its __generalization__.\n",
    "\n",
    "One way to evaluate this is by carrying out __cross validation (CV)__, which is a technique used to test the effectiveness of a machine learning models, as well as a re-sampling procedure used to evaluate a model if we have a limited data. One method of cross-validation can be done by splitting our dataset into two sets: a __training set__ and a __test set__. The training dataset is used to train our model, and we used the remaining data in the test set to evaluate the generalization of the model. Since the model has never seen the test set before, this is representative of applying the model to other data it would encounter in the real-world.\n",
    "\n",
    "Let us consider the simple example for linear regression below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import get_regression_data, visualise_regression_data\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and test datasets of input-output pairs\n",
    "\n",
    "def split_data(X, Y, train_portion, test_portion):\n",
    "    data = [(x, y) for x, y in zip(X, Y)]\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "    n = len(data)\n",
    "    n_train = round(n*train_portion)\n",
    "    \n",
    "    train_data = data[0:n_train]\n",
    "    test_data = data[n_train:]\n",
    "    return train_data, test_data\n",
    "\n",
    "def get_Y(data_pairs):\n",
    "    return np.array([pair[1] for pair in data_pairs])\n",
    "def get_X(data_pairs):\n",
    "    return np.array([pair[0] for pair in data_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting our regression model\n",
    "X, Y = get_regression_data()\n",
    "train_data, test_data = split_data(X, Y, 0.8, 0.2)\n",
    "\n",
    "reg = LinearRegression().fit(get_X(train_data), get_Y(train_data))\n",
    "Y_pred = reg.predict(get_X(train_data))\n",
    "visualise_regression_data(get_X(train_data), get_Y(train_data), Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fit our regression model onto our training data, we can evaluate its performance using the test set via two metrics, the MSE, that measures the average squared differences between the prediction and label, defined as:\n",
    "\n",
    "$$MSE(y_{true}, y_{pred}) = \\frac{1}{n_{samples}}\\sum (y_{true} - y_{pred})^{2}$$\n",
    "\n",
    "As well as the $R^{2}$ coefficient, which represents the proportion of variance in the outcome of our model is capable of predicting based on its features, defined as:\n",
    "\n",
    "$$R^{2}(Y_{true}, Y_{pred}) = 1 - \\frac{\\sum (y_{true} - y_{pred})^{2}}{\\sum (y_{true} - \\bar{y})^{2}}$$\n",
    "\n",
    "We show these below for our regression model on data that our model has not encountered, which is our test dataset. We can also compare these metrics to how the model does with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating performance on test data\n",
    "\n",
    "def MSE(y_true, y_pred):\n",
    "    diff = (y_true - y_pred)**2\n",
    "    return np.mean(diff)\n",
    "\n",
    "def R2(y_true, y_pred):\n",
    "    y_bar = np.mean(y_pred)\n",
    "    diff_pred = np.sum((y_true - y_pred)**2)\n",
    "    diff_mean = np.sum((y_true - y_bar*np.ones(y_true.shape[0]))**2)\n",
    "    r2 = 1 - (diff_pred/diff_mean)\n",
    "    return r2\n",
    "\n",
    "# Training data\n",
    "print(\"Training data:\")\n",
    "print(\"MSE:\", MSE(get_Y(train_data), Y_pred))\n",
    "print(\"R2:\", R2(get_Y(train_data), Y_pred))\n",
    "print()\n",
    "\n",
    "# Test data\n",
    "Y_pred = reg.predict(get_X(test_data))\n",
    "print(\"Test data:\")\n",
    "print(\"MSE:\", MSE(get_Y(test_data), Y_pred))\n",
    "print(\"R2:\", R2(get_Y(test_data), Y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same logic can be applied to classification models. Let us import the iris dataset and fit a logistic regression model to it. This dataset has four input features and three possible classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting model to our data\n",
    "X, Y = load_iris(return_X_y=True)\n",
    "train_data, test_data = split_data(X, Y, 0.8, 0.2)\n",
    "X_train, Y_train = np.array(get_X(train_data)), np.array(get_Y(train_data))\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X_train, Y_train)\n",
    "Y_pred = clf.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have fit our logistic regression model to the iris data, we can apply our metrics to measure the scores/error of the model. For classification models, the simplest metric is __accuracy__, which is given as follows:\n",
    "\n",
    "$$accuracy = \\frac{\\text{correct predictions}}{all predictions}$$\n",
    "\n",
    "We can also measure __precision__, given as follows:\n",
    "\n",
    "$$precision = \\frac{\\text{true positives}}{\\text{true positives + false positives}}$$\n",
    "\n",
    "These two metrics are computed below for the training set and the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing accuracy and precision of our model on both training and test datasets\n",
    "def accuracy(Y_true, Y_pred):\n",
    "    correct_count = 0\n",
    "    for y_true, y_pred in zip(Y_true, Y_pred):\n",
    "        if y_true == y_pred:\n",
    "            correct_count += 1\n",
    "    accuracy = correct_count/Y_true.shape[0]\n",
    "    return accuracy\n",
    "\n",
    "def precision(Y_true, Y_pred):\n",
    "    class_list = list(set(Y_true))\n",
    "    \n",
    "    true_positives = np.zeros(len(class_list))\n",
    "    false_positives = np.zeros(len(class_list))\n",
    "    for y_true, y_pred in zip(Y_true, Y_pred):\n",
    "        for idx, label in enumerate(class_list):\n",
    "            if y_pred == label:\n",
    "                if y_pred == y_true:\n",
    "                    true_positives[idx]+=1\n",
    "                else:\n",
    "                    false_positives[idx]+=1\n",
    "    precision = np.sum(true_positives)/(np.sum(true_positives) + np.sum(false_positives))\n",
    "    return precision\n",
    "\n",
    "# Training data\n",
    "print(\"Training data:\")\n",
    "print(\"accuracy:\", accuracy(Y_pred, Y_train))\n",
    "print(\"precision:\", precision(Y_pred, Y_train))\n",
    "print()\n",
    "\n",
    "# Test data\n",
    "X_test, Y_test = np.array(get_X(test_data)), np.array(get_Y(test_data))\n",
    "Y_pred = clf.predict(X_test)\n",
    "print(\"Test data:\")\n",
    "print(\"accuracy:\", accuracy(Y_pred, Y_test))\n",
    "print(\"precision:\", precision(Y_pred, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it. We see that our models even performs well on data it had never seen before, validating the capacity of the model to generalize. However, if we have a small dataset, this approach may result in bias, as the model misses information that it did not receive during training. Hence, we introduce another method of cross-validation that generally results in less bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold Cross-Validation\n",
    "\n",
    "Another method for cross-validation is the __k-fold cross-validation__ method, which generally ensures lower bias, as it allows for every observation to appear in both the training and test datasets. This approach is especially useful when we have a limited amount of data.\n",
    "\n",
    "How do we apply this method? We split our data into _k_ __folds__, each with an equal number of observations. We then assign k-1 folds to the training dataset, and test our trained model on the k fold that was not used to train the model. We alternate which of the k folds is used as a test set until all folds have been both used in the training and test datasets. For each round of training, we calculate the metric, and this method returns the average metrics calculated for each round of training. We show how this method can be applied below:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/523/1*C5FJt_NH1BWJrFSvw_6jtw.png\" width=\"500px\" height=\"500px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits data into k_folds\n",
    "def k_split(X, Y, k_folds):\n",
    "    fold_size = int(Y.shape[0]/k_folds)\n",
    "    X_folds, Y_folds = [], []\n",
    "    for i in range(k_folds):\n",
    "        try:\n",
    "            X_folds.append(X[i*fold_size:(i+1)*fold_size])\n",
    "            Y_folds.append(Y[i*fold_size:(i+1)*fold_size])\n",
    "        except:\n",
    "            X_folds.append(X[i*fold_size:])\n",
    "            Y_folds.append(Y[i*fold_size:])\n",
    "    \n",
    "    return X_folds, Y_folds\n",
    "        \n",
    "# Computes average metrics by using all folds for testing and training separately\n",
    "def k_fold_validation(X, Y, k_folds, metrics, model):\n",
    "    metric_values = np.zeros((k_folds, len(metrics))) # k values per metric\n",
    "    \n",
    "    for k in range(k_folds):\n",
    "        X_folds, Y_folds = k_split(X, Y, k_folds) # resplit our data into k_folds\n",
    "        X_test = X_folds[k]\n",
    "        Y_test = Y_folds[k]\n",
    "        del X_folds[k]\n",
    "        del Y_folds[k]\n",
    "        \n",
    "        # remaining elements are part of the training dataset\n",
    "        X_train = np.array([observation for fold in X_folds for observation in fold])\n",
    "        Y_train = np.array([label for fold in Y_folds for label in fold])\n",
    "        \n",
    "        # fitting our chosen model to the selected training data\n",
    "        model_fit = model.fit(X_train, Y_train)\n",
    "        Y_pred = model_fit.predict(X_test)\n",
    "        for idx, metric in enumerate(metrics):\n",
    "            metric_values[k, idx] = metric(Y_test, Y_pred)\n",
    "    metric_values = np.mean(metric_values, axis=0)\n",
    "    \n",
    "    return metric_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression k-fold cross validation\n",
    "X, Y = get_regression_data()\n",
    "metric_values = k_fold_validation(X, Y, 2, (MSE, R2), LinearRegression())\n",
    "print(metric_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification k-fold cross-validation\n",
    "X, Y = load_iris(return_X_y=True)\n",
    "metric_values = k_fold_validation(X, Y, 8, (accuracy, precision), LogisticRegression(random_state=0))\n",
    "print(metric_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn also provides us with an iterator to help us carry out k-fold cross-validation, as shown below for the regression case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X, Y = get_regression_data()\n",
    "kf = KFold(n_splits=3)\n",
    "MSEs = np.array([])\n",
    "R2s = np.array([])\n",
    "for train, test in kf.split(X):\n",
    "    X_train, X_test, Y_train, Y_test = X[train], X[test], Y[train], Y[test]\n",
    "    reg = LinearRegression().fit(X_train, Y_train)\n",
    "    Y_pred = reg.predict(X_test)\n",
    "    MSEs = np.append(MSEs, MSE(Y_test, Y_pred))\n",
    "    R2s = np.append(R2s, R2(Y_test, Y_pred))\n",
    "\n",
    "print(\"MSE:\", np.mean(MSEs))\n",
    "print(\"R2s:\", np.mean(R2s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding on k-fold cross validation, we can also carry out __repeated k-fold cross-validation__, where we repeat the k-fold cross validation process _n_ times, where the folds are different in every iteration, then take the average metric."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
