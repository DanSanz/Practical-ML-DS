{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Intro\n",
    "\n",
    "As you should know by now, most tasks can be either defined as regression (predicting a continuous value e.g. house price) or classification (predicting a discrete value e.g. cat vs dog in images) problems.\n",
    "\n",
    "The simplest form of a classification problem is binary classification, where an example either belongs to a class or doesn't and there is only one class. That is, every example has a lable which is either true or false.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_binary_data(m=50): \n",
    "    X = np.random.randn(m) #sample from a normal distribution\n",
    "    X = np.array(sorted(X))\n",
    "    Y = X > 0.2    # return binary vector with true where X above some threshold and false if below\n",
    "    return X, Y #returns X (the input) and Y (labels)\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    plt.figure()\n",
    "    plt.scatter(X, Y, c='r', marker='x')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "X, Y = make_binary_data()\n",
    "plot_data(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, the output of our model could be any real number, negative or positive, unbounded in magnitude. \n",
    "In classification, we can interpret the model output as a confidence (probability) that the example belongs to a particular class. \n",
    "So for a classification task, any real numbered output value might not make sense.\n",
    "Instead, to represent a class confidence we need to bound the output within the range 0-1.\n",
    "We can do this by applying a **sigmoid** function to the output.\n",
    "\n",
    "![](images/sigmoid.jpg)\n",
    "\n",
    "We can also write a function to compute the derivative of the sigmoid function.\n",
    "We will need this to differentiate our loss with respect to the model parameters, as they only affect the loss through the sigmoid.\n",
    "\n",
    "![](images/sigmoid_deriv.jpg)\n",
    "\n",
    "Let's implement our sigmoid function in code, and let's also include a boolean argument which returns us the gradient if true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z, grad=False):\n",
    "    if grad: ## if i ask for the function gradient rather than the function evaluation\n",
    "        return (1-sigmoid(z))*sigmoid(z) ## return sigmoid gradient\n",
    "    return 1 / (np.exp(-z) + 1) ## else return sigmoid gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build our binary classifier model class\n",
    "\n",
    "Stacking this transformation on the end of our usual linear transformation changes the model, and hence the gradient of the loss function.\n",
    "We'll use the chain rule to compute the gradient of these composed linear and sigmoid functions.\n",
    "\n",
    "![](images/binary_classification.jpg)\n",
    "\n",
    "## Logits\n",
    "Our models produce a value that is input a sigmoid or other type of function that converts it into a probability distribution.\n",
    "We call this value a **logit**.\n",
    "\n",
    "Note: mathematically, the term logit refers to the log of the odds of a probability. \n",
    "That makes the logit function the inverse of the sigmoid.\n",
    "However, we will use the term logit later to refer to the values in models that are passed to other functions that convert these unnormalised values into probability distributions. An example of a function that does this other than the sigmoid function is the softmax function, which we will cover in the next notebook.\n",
    "\n",
    "## Implementation from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier:\n",
    "    def __init__(self, n_features):\n",
    "        self.w = np.random.rand(n_features)\n",
    "        self.b = np.random.rand()\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        x = self.w * x + self.b\n",
    "        x = sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def update_params(self, new_w, new_b):\n",
    "        self.w = new_w\n",
    "        self.b = new_b    \n",
    "\n",
    "    def calc_deriv(self, X, y_hat, labels):\n",
    "        m = len(Y) # m = number of examples\n",
    "        diffs = y_hat - labels # calculate errors\n",
    "        # dzdw = 2*np.array(np.sum(diffs*X) / m) # calculate derivative of loss with respect to weights\n",
    "        # dzdb = 2*np.array(np.sum(diffs) / m) # calculate derivative of loss with respect to bias\n",
    "        dzdb = 1\n",
    "        dzdw = X\n",
    "\n",
    "        dhdz = sigmoid(y_hat, grad=True)\n",
    "\n",
    "        dhdw = dhdz*dzdw\n",
    "        dhdb = dhdz*dzdb\n",
    "\n",
    "        return dhdw, dhdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The loss function for binary classification\n",
    "\n",
    "### What is cross entropy?\n",
    "\"Any loss term that contains a negative log-likelihood is a cross entropy between the empirical distribution defined by the training set and the probability distribution defined by the model\" (Goodfellow et al, 2016)\n",
    "\n",
    "The aim of machine learning is to be able to model things that we care about. \n",
    "We want to be able to know the distribution of features (unsupervised problems) and labels (supervised problems) that appear in the data.\n",
    "That is, we want to know the true data generating distribution.\n",
    "Maximum likelihood implicitly minimises the difference between the empirical distribution $\\hat{p}_{data}$ and the model distribution $p_{model}$.\n",
    "This is because what we minimise is the same term as that which appears in the equation for the KL-divergence, which is an expression which explicitly measures the difference between two distributions, once the terms which do not depend on our model parameterisation are removed.\n",
    "\n",
    "# show KL-divergence\n",
    "\n",
    "Unlike in regression problems, how we measure the loss for a particular example in a classification problem depends on what the label is. That is, if the label is 1 (yes, it belongs to the positive class) then we want to penalise it more for having a prediction nearer to 0 (no, it belongs to the negative class), and vice versa.\n",
    "\n",
    "### Binary cross entropy\n",
    "\n",
    "![](./images/bce.jpg)\n",
    "[Cross entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy)\n",
    "\n",
    "Let's implement our binary cross entropy loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BinaryCrossEntropyLoss(prediction, label, grad=False):\n",
    "    if grad:\n",
    "        if label == 0:\n",
    "            return (1-label) / (1-prediction)\n",
    "        else:\n",
    "            return - (label / prediction)\n",
    "    return - ( label*np.log(prediction) + (1 - label) * np.log(1 - prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put all of this together to implement our training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "H = Classifier(n_features=1)\n",
    "\n",
    "# PLOT OUR HYPOTHESIS BEFORE TRAINING\n",
    "plt.figure()\n",
    "plt.title('Before training')\n",
    "plt.ylabel('Label')\n",
    "plt.xlabel('Features')\n",
    "plt.scatter(X, H(X), label='predictions')\n",
    "plt.scatter(X, Y, c='r', marker='x', label='ground truth')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "epochs = 1000\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    epoch_losses = []\n",
    "    for x, y in zip(X, Y):\n",
    "        prediction = H(x)\n",
    "        loss = BinaryCrossEntropyLoss(prediction, y)\n",
    "        epoch_losses.append(loss)\n",
    "        dhdw, dhdb = H.calc_deriv(x, prediction, y)\n",
    "        dLdh = BinaryCrossEntropyLoss(prediction, y, grad=True)\n",
    "        dLdw = dLdh * dhdw\n",
    "        dLdb = dLdh * dhdb\n",
    "        new_w = H.w - learning_rate * dLdw\n",
    "        new_b = H.b - learning_rate * dLdb\n",
    "        H.update_params(new_w, new_b)\n",
    "    losses.append(np.mean(epoch_losses))\n",
    "        \n",
    "# PLOT THE LOSS CURVE AND OUR HYPOTHESIS AFTER TRAINING\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_title('Loss curve')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.plot(losses)\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.set_title('After Training')\n",
    "ax2.set_ylabel('Label')\n",
    "ax2.set_xlabel('Features')\n",
    "ax2.scatter(X, H(X), label='predictions')\n",
    "ax2.scatter(X, Y, c='r', marker='x', label='ground truth')\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing classification in SKLearn\n",
    "\n",
    "SkLearn is a Python library that has a load of tools for data science and modelling.\n",
    "It contains model classes with simple APIs that allow us to train and test them easily without starting from scratch.\n",
    "\n",
    "SkLearn is an industry-standard tool for traditional machine learning techniques like this.\n",
    "We will use it throughout this chapter to show how to quickly implement any of the models that we cover.\n",
    "However, we will see later that SkLearn is not capable of allowing us to imlement much more powerful [deep models].\n",
    "\n",
    "Let's now use SkLearn to implement the same logistic regression algorithm as above, but with some significant abstraction.\n",
    "\n",
    "Check out the docs [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "myLogisticModel = sklearn.linear_model.LogisticRegression()\n",
    "\n",
    "print(X.shape, Y.shape)\n",
    "Y_ = np.squeeze(Y) #remove empty dimensions\n",
    "myLogisticModel.fit(X, Y_, solver='sag') # fit the model using SGD (a solver called sag in sklearn (Stochastic Average Gradient))\n",
    "plt.scatter(X, myLogisticModel.predict(X)) # plot predictions\n",
    "plt.scatter(X, Y, marker='x', c='r')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- the labels for classification problems should be an integer representing the index of the class which that example belongs to\n",
    "- binary cross entropy is a new differentiable loss function that can be optimised to solve classification problems\n",
    "- binary classification can be implemented by having a single boolean integer label for each example, where 1 represents it being a member of that class and 0 represents it not being a member of that class\n",
    "\n",
    "## Next steps:\n",
    "- [Multiclass classification]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
